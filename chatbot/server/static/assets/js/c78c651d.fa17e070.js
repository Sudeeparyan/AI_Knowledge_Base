"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[8957],{12601:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>l});var r=t(74848),s=t(28453);const o={},a=void 0,i={id:"RAG360/Evaluation/Metrics/Custom Metric",title:"Custom Metric",description:"Custom Metric Creation Workflow in Trulens",source:"@site/docs/RAG360/Evaluation/Metrics/Custom Metric.md",sourceDirName:"RAG360/Evaluation/Metrics",slug:"/RAG360/Evaluation/Metrics/Custom Metric",permalink:"/docs/RAG360/Evaluation/Metrics/Custom Metric",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Metrics",permalink:"/docs/category/metrics"},next:{title:"System Metrics vs Module Metrics",permalink:"/docs/RAG360/Evaluation/Metrics/System Metrics vs Module Metrics"}},c={},l=[{value:"Custom Metric Creation Workflow in Trulens",id:"custom-metric-creation-workflow-in-trulens",level:2},{value:"Prerequisite Knowledge",id:"prerequisite-knowledge",level:3},{value:"Procedure",id:"procedure",level:3},{value:"Reference",id:"reference",level:3},{value:"Custom Metrics in Astro Chat Assistant",id:"custom-metrics-in-astro-chat-assistant",level:2},{value:"Custom Metrics in Chat Assistant",id:"custom-metrics-in-chat-assistant",level:3},{value:"Extended Custom Metric Class from LLM Provider",id:"extended-custom-metric-class-from-llm-provider",level:3},{value:"Trulens Recorder Implementation",id:"trulens-recorder-implementation",level:3},{value:"Feedback Definitions and Recording",id:"feedback-definitions-and-recording",level:3},{value:"Custom Metrics in Datasheet Assistant",id:"custom-metrics-in-datasheet-assistant",level:2},{value:"Metrics Definition with Prompts",id:"metrics-definition-with-prompts",level:3}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"custom-metric-creation-workflow-in-trulens",children:"Custom Metric Creation Workflow in Trulens"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisite-knowledge",children:"Prerequisite Knowledge"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Feedback Functions in Trulens"}),"\n",(0,r.jsx)(n.li,{children:"Providers in Trulens"}),"\n",(0,r.jsx)(n.li,{children:"Records in Trulens"}),"\n",(0,r.jsx)(n.li,{children:"Selectors in Trulens"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"procedure",children:"Procedure"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create a python class that inherits the desired provider(OpenAI or\nAzureOpenAI) class."}),"\n",(0,r.jsx)(n.li,{children:"In the class, define a function that takes inputs and provides any float\nvalue between 0 to 1 as the output."}),"\n",(0,r.jsx)(n.li,{children:"The function can be purely logical or can use AI to come up with the value of\nthe metric between 0 to 1."}),"\n",(0,r.jsx)(n.li,{children:"Example of a purely logical function/metric - Identifying lint errors in a\ncode snippet using python libraries and producing a score based on the number\nof errors and the number of lines in the code."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'def _lint_check_cpp(self, file_path: str):\n    import subprocess\n\n    result = subprocess.run(\n        ["poetry", "run", "cpplint", file_path],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    number_of_errors = result.stdout.decode("utf-8").split("\\n")\n    error_number = 0\n    for line in number_of_errors:\n        if "Total errors found" in line:\n            errors = int(line.split(":")[1].strip())\n            if errors == 0:\n                error_number = 0\n            else:\n                error_number = errors\n    error_list = result.stderr.decode("utf-8")\n    return error_number, error_list\n\ndef lint_check(self, candidate: str, language: str):\n    if language == "cpp":\n        with open(TEST_CPP_FILE_PATH, "w") as f:\n            f.write(candidate)\n        result = self._lint_check_cpp(TEST_CPP_FILE_PATH)\n        if result[0] == 0:\n            return 1.0, {"reason": "No linting errors found"}\n        else:\n            number_of_lines = len(candidate.split("\\n"))\n            linting_score = 1 - (result[0] / number_of_lines)\n            if linting_score < 0:\n                linting_score = 0.0\n            return linting_score, {\n                "reason": (\n                    f"{\'Number of Errors: \' + str(result[0])}\\n"\n                    f"{\'List of Errors: \' + str(result[1])}"\n                )\n            }\n'})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsx)(n.li,{children:"Example of using AI to come up with scores: Calculating how accurate a\ngenerated search query is with respect to the actual query."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'SEARCH_QUERY_ACCURACY_TEMPLATE = """\n\nSystem:\nYou are a SEARCH QUERY ACCURACY classifier providing the accuracy of the SEARCH QUERY formed by combination of USER QUESTION and CHAT HISTORY.\nYou will be provided with a USER QUESTION, CHAT HISTORY and a SEARCH QUERY.\nYour task is to evaluate If the SEARCH QUERY accurately reflects the USER QUESTION and includes all necessary details to be understood on its own. This includes appropriately filling in any gaps left by an incomplete USER QUESTION using the CHAT HISTORY.\nPurpose of the SEARCH QUERY is to retrieve the desired information from the database related to the USER QUESTION.\n\nUSER QUESTION (mentioned within the triple quotes below):\n```{user_question}```\n\nCHAT HISTORY (mentioned within the triple quotes below):\n```{chat_history}```\n\nSEARCH QUERY (mentioned within the triple quotes below):\n```{search_query}```\n\nPlease answer using the entire template below.\n\nTEMPLATE:\nObservation: <The observation on the criteria for the SEARCH QUERY ACCURACY evaluation.>\nScore: <The score 0-10 based on the given criteria>\nSupporting Evidence: <Provide your reasons for scoring based on the ACCURACY of the SEARCH QUERY. Tie it back to the evaluation being completed.>\n"""\ndef search_query_accuracy_with_cot_reasons(\n    self, user_question: str, chat_history: list[str], search_query: str\n) -> float:\n    """Get the accuracy score for the search query generated\n\n    Usage:\n        search_query_accuracy_feedback = (\n            Feedback(provider.search_query_accuracy_with_cot_reasons, name="Search Query Accuracy")\n            .on(user_question=Select.RecordCalls.query.args["question"])\n            .on(chat_history=Select.RecordCalls.query.args["chat_history"])\n            .on(search_query=Select.RecordCalls.get_search_query.rets)\n        )\n\n\n        Args:\n            user_question (str): The user question for which the query is being generated\n            chat_history (list[str]): previous chat history\n            search_query (str): The search query generated by the system\n\n        Returns:\n            float: A value between 0 and 1. 0 being "not accurate" and 1 being "mostly accurate".\n    """\n\n    system_prompt = str.format(\n        SEARCH_QUERY_ACCURACY_TEMPLATE,\n        user_question=user_question,\n        chat_history=chat_history,\n        search_query=search_query,\n    )\n    return self.generate_score_and_reasons(system_prompt)\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The format of output given in the prompt is important because Trulens will\nhelp in visualising the output given for 'Supporting Evidence' by the LLM."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The generate_score_and_reasons() function will help in parsing the generated\ntext output of LLM, store the metrics and reasons that will help in\nvisualizing them in the dashboard later."}),"\n"]}),"\n"]})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Once we have a function that will provide us a score, we can implement it in\nevaluation by wrapping it as a Feedback Function."}),"\n",(0,r.jsx)(n.li,{children:"Feedback Functions are individual metrics in Trulens terms."}),"\n",(0,r.jsx)(n.li,{children:"Feedback Function definition for the above examples:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'search_query_accuracy_feedback = Feedback(\n  provider.search_query_accuracy_with_cot_reasons,\n  (name = "Search Query Accuracy"),\n)\n  .on((user_question = Select.RecordCalls.query.args["question"]))\n  .on((chat_history = Select.RecordCalls.query.args["chat_history"]))\n  .on((search_query = Select.RecordCalls.get_search_query.rets));\n\nlint_check = Feedback(provider.lint_check, (name = "Lint Check"))\n  .on_output()\n  .on(Select.RecordCalls.get_candidate.args["language"]);\n'})}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsx)(n.mdxAdmonitionTitle,{}),(0,r.jsx)(n.p,{children:"Note that the arguments to the python metric/function we defined will be\nmapped using the on() method and Select object in Trulens which will make use of\nattributes of the Record object."})]}),"\n",(0,r.jsxs)(n.ol,{start:"9",children:["\n",(0,r.jsx)(n.li,{children:"Now the custom metric can be calculated just as any inbuilt Trulens metric."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.trulens.org/trulens_eval/evaluation/feedback_implementations/custom_feedback_functions/",children:"Reference"})})}),"\n",(0,r.jsx)(n.h2,{id:"custom-metrics-in-astro-chat-assistant",children:"Custom Metrics in Astro Chat Assistant"}),"\n",(0,r.jsx)(n.h3,{id:"custom-metrics-in-chat-assistant",children:"Custom Metrics in Chat Assistant"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Factors"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Valid Number of Function Arguments while using Skills"}),(0,r.jsx)("td",{children:"Validate if the proper number of arguments were provided for the selected skill."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Correctness of Skill/Function Call made by the Orchestrator"}),(0,r.jsx)("td",{children:"Check if the choice of skill made by the Orchestrator is the desired skill to answer the given query."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Guideline Adherence of the Response"}),(0,r.jsx)("td",{children:"Check if the response generated by LLM follows the guidelines provided in the System Prompt during synthesis."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Average Moderation Score from OpenAI for the response"}),(0,r.jsx)("td",{children:"Average of scores provided by Moderation [Hate, Violence, and Harassment/Threatening] API of OpenAI."})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"extended-custom-metric-class-from-llm-provider",children:"Extended Custom Metric Class from LLM Provider"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'class FunctionsEvaluation(Provider):\n    provider: Provider\n\n    def __init__(self, provider: Provider, **kwargs):\n        """Initialize the FunctionsEvaluation"""\n        super().__init__(provider=provider, **kwargs)\n\n    def evaluate_function_arguments(self, function_arguments, skill_arguments):\n        """Evaluate the function arguments"""\n        correct_function_arguments = 0\n        for function in function_arguments:\n            function_name = function["function"]["name"]\n            function_args = list(json.loads(function["function"]["arguments"]).keys())\n            if function_args == skill_arguments[function_name]:\n                correct_function_arguments += 1\n        if correct_function_arguments == len(function_arguments) and correct_function_arguments > 0:\n            return 1.0\n        return 0.0\n\n    def evaluate_function_choice(self, query, function_arguments, skill_arguments):\n        """Evaluate the function choice"""\n        correctness_scores = []\n        if len(function_arguments) == 0:\n            return [\'{"Correctness": 0}\']\n        for function in function_arguments:\n            function_name = function["function"]["name"]\n            correctness_scores.append(\n                self.provider._create_chat_completion(\n                    prompt=f"{FUNCTION_CHOICE_EVAL_SYSTEM_PROMPT}{FUNCTION_CHOICE_EVALUATION_PROMPT.format(query=query, function_description=skill_arguments[function_name])}"  # noqa\n                )\n            )\n        return correctness_scores\n\n    def evaluate_function_choice_with_cot_reasons(self, query, function_arguments, skill_arguments):\n        """Evaluate the function choice with cot reasons"""\n        correctness_score = []\n        reason = self.evaluate_function_choice(query, function_arguments, skill_arguments)\n        for function in reason:\n            function = json.loads(function)\n            correctness_score.append(re_0_10_rating(str(function["Correctness"])) / 10)\n        return np.mean(correctness_score), {"reason": reason}\n\n    def evaluate_response_guideline_adherence(self, query, response, function_responses):\n        """Evaluate the response guideline adherence"""\n        adherence_scores = []\n        reasons = []\n        mean = 0.0\n        if len(function_responses) > 0:\n            context = "\\n".join(function_responses)\n        else:\n            context = "No Context"\n        for guideline, criterion in zip(guidelines, criteria):\n\n            reason = self.provider._create_chat_completion(\n                prompt=f"{GUIDELINES_ADHERENCE_EVAL_SYSTEM_PROMPT}{GUIDELINE_ADHERENCE_EVALUATION_PROMPT.format(query=query, response=response, context=context, criteria=criterion,guideline=guideline,)}",  # noqa\n            )\n            reason = json.loads(reason)\n            reasons.append(\n                {\n                    "Guideline": guideline,\n                    "CriterionCompliance": reason["CriterionCompliance"],\n                    "Feedback": reason["Feedback"],\n                    "Adherence": reason["Adherence"],\n                }\n            )\n            if reason["Adherence"] == "YES":\n                adherence_scores.append(1)\n            elif reason["Adherence"] == "NO":\n                adherence_scores.append(0)\n\n        reasons_string = "\\n".join([json.dumps(reason) for reason in reasons])\n        if len(adherence_scores) > 0:\n            mean = round(np.mean(adherence_scores), 2)\n        return mean, reasons_string\n\n    def evaluate_guidelines_adherence_with_cot_reasons(self, query, response, function_responses):\n        """Evaluate the function choice with cot reasons"""\n        score, reason = self.evaluate_response_guideline_adherence(\n            query, response, function_responses\n        )\n        return float(score), {"reason": reason}\n\n    def evaluate_moderation(self, response):\n        """Evaluate the moderation hate"""\n        score = {}\n        score["ModerationHate"] = round(1 - self.provider.moderation_hate(response), 2)\n        score["ModerationViolence"] = round(1 - self.provider.moderation_violence(response), 2)\n        score["ModerationHarassmentThreatening"] = round(\n            1 - self.provider.moderation_harassment_threatening(response), 2\n        )\n        mean_score = round(np.mean(list(score.values())), 2)\n        return mean_score, {"reason": score}\n\n'})}),"\n",(0,r.jsx)(n.h3,{id:"trulens-recorder-implementation",children:"Trulens Recorder Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'class OrchestratorTest:\n    """Class for TruCustomApp"""\n\n    async def get_answer(self, orchestrator, task):\n        return await orchestrator.run(task)\n\n    @instrument\n    def query(self, question: str, chat_history: List[str]) -> str:\n        """Get the answer to the question"""\n        chatHistory: List[str] = chat_history\n        ignoredDataSources: List[str] = []\n        metaData: TaskMetaData = TaskMetaData()\n        attachments: List[Attachment] = []\n        additional_input: TaskAdditionalInput = TaskAdditionalInput(\n            chatHistory=chatHistory,\n            ignoredDataSources=ignoredDataSources,\n            metaData=metaData,\n            attachments=attachments,\n        )\n\n        task = TaskRequestBody(\n            input=question,\n            additional_input=additional_input,  # noqa\n        )\n\n        # Create an instance of OpenAIFunctionCalling\n        orchestrator = get_orchestrator_from_config(debug_mode=True)\n        result = asyncio.run(self.get_answer(orchestrator, task))\n        _ = self.get_function_args(orchestrator.performance_evaluation_log)\n        _ = self.get_skills_responses(orchestrator.performance_evaluation_log)\n        _ = self.get_skill_args(\n            orchestrator.performance_evaluation_log["skill_required_arguments_schema"]\n        )\n        _ = self.get_skill_descriptions(\n            orchestrator.performance_evaluation_log["skill_required_arguments_schema"]\n        )\n        return result.answer\n'})}),"\n",(0,r.jsx)(n.h3,{id:"feedback-definitions-and-recording",children:"Feedback Definitions and Recording"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'f_open_ai = fOpenAI(\n    model_engine=os.getenv("OPEN_AI_CHAT_MODEL"),\n    api_key=os.getenv("OPEN_AI_API_KEY"),\n)\nfn_eval = FunctionsEvaluation(provider=f_open_ai)\nf_fn_args_eval = (\n   Feedback(\n       fn_eval.evaluate_function_arguments,\n       name="Function Arguments Evaluation",\n   )\n   .on(Select.RecordCalls.get_function_args.rets)\n   .on(Select.RecordCalls.get_skill_args.rets)\n)  # instantiate feedback function\nf_moderation = Feedback(fn_eval.evaluate_moderation, name="Moderation Evaluation").on_output()\ngrounded = Groundedness(groundedness_provider=f_open_ai)\n# Define a groundedness feedback function\nf_groundedness = (\n    Feedback(grounded.groundedness_measure_with_cot_reasons, name="Groundedness")\n    .on(Select.RecordCalls.get_skills_responses.rets)\n    .on_output()\n    .aggregate(grounded.grounded_statements_aggregator)\n)\nf_qa_relevance = (\n    Feedback(f_open_ai.relevance_with_cot_reasons, name="Answer Relevance")\n    .on_input()\n    .on_output()\n)\nf_fn_choice_eval = (\n    Feedback(\n       fn_eval.evaluate_function_choice_with_cot_reasons,\n       name="Function Choice Evaluation",\n    )\n    .on_input()\n    .on(Select.RecordCalls.get_function_args.rets)\n    .on(Select.RecordCalls.get_skill_descriptions.rets)\n)\nf_fn_guideline_adherence = (\n    Feedback(\n        fn_eval.evaluate_guidelines_adherence_with_cot_reasons,\n        name="Response Guideline Adherence",\n     )\n     .on_input()\n     .on_output()\n     .on(Select.RecordCalls.get_skills_responses.rets)\n)\n\nwith tru_custom_app as recording:\n    orchestrator.query(question, chat)\nrecord = recording.get()\nfeedback_results = tru.run_feedback_functions(\n    record=record,\n    feedback_functions=[\n        f_fn_args_eval,\n        f_groundedness,\n        f_qa_relevance,\n        f_fn_choice_eval,\n        f_fn_guideline_adherence,\n        f_moderation,\n   ],\n)\ntru.add_feedbacks(feedback_results)\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"custom-metrics-in-datasheet-assistant",children:"Custom Metrics in Datasheet Assistant"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Metrics"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Expected Answer Match or Conceptual Information Overlap"}),(0,r.jsx)("td",{children:"This metric compares the overlap of information between a source and a statement. It returns a score between 0 and 1, indicating the degree of overlap."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Conversation Starter Presence"}),(0,r.jsx)("td",{children:"This metric evaluates the presence of conversation starters in a given text. It returns a score between 0 and 1, indicating the presence and absence of conversation starters respectively."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Search Query Accuracy"}),(0,r.jsx)("td",{children:"This metric evaluates the accuracy of a search query generated based on a user question and chat history. It returns a score between 0 and 1, indicating the accuracy of the search query."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{class:"custom-header",children:"Search Query Conciseness"}),(0,r.jsx)("td",{children:"This metric evaluates the conciseness of a search query generated based on a user question and chat history. It returns a score between 0 and 1, indicating the conciseness of the search query."})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"metrics-definition-with-prompts",children:"Metrics Definition with Prompts"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'COMPARE_ANSWER_TEMPLATE = """\nSystem:\nYou are an INFORMATION / CONCEPTUAL OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\nUse the \'Conceptual Overlap\' model to compare the given actual answer with given expected answer to see how much does actual answer matches.\nRespond only as a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping\n\nA few additional scoring guidelines:\n\n- Long RESPONSES should score equally well as short RESPONSES.\n\n- Score should increase as the RESPONSE contains more information from the EXPECTED RESPONSE.\n\n- RESPONSE that does not contain any overlapping information from the EXPECTED RESPONSE should get a score of 0.\n\n- RESPONSE that contains some overlapping information from the EXPECTED RESPONSE should get as score of 2, 3, or 4. Higher score indicates more overlapping informations.\n\n- RESPONSE that contains more overlapping information from the EXPECTED RESPONSE should get a score between a 5, 6, 7 or 8. Higher score indicates more overlapping informations.\n\n- RESPONSE that almost contains most of the overlapping information from the EXPECTED RESPONSE should get a score of 9 or 10.\n\n- RESPONSE must contain all informations present in the EXPECTED RESPONSE to get a score of 10.\n\n- RESPONSE might contain additional information that is not present in the EXPECTED RESPONSE. This should not affect the score.\n\n- RESPONSE that confidently FALSE should get a score of 0.\n\n- RESPONSE that is only seemingly MATCH should get a score of 0.\n\n\n- Never elaborate.\n\nEXPECTED RESPONSE (mentioned within the triple quotes below):\n```{expected_answer}```\n\nRESPONSE (mentioned within the triple quotes below):\n```{actual_answer}```\n\nScore: """\n\n\nCONVERSATIONAL_WORDS_PRESENCE_TEMPLATE = """\nSystem:\nYou are a CONVERSATION WORDS PRESENCE classifier providing the absence of CONVERSATION WORDS in the given TEXT.\nYou will be provided with a TEXT.\nYour task is to evaluate the absence of CONVERSATION WORDS in the TEXT.\nConversation starters are informal phrases or questions that are used to initiate or continue a conversation. They are not directly related to the main topic of the text.\n\nPlease follow these guidelines for your evaluation:\n1. Give a score of 0 if the SEARCH QUERY contains informal language, conversation starters.\n2. Give a score of 10 if the SEARCH QUERY does not contain informal language, conversation starters.\n\nTEXT (mentioned within the triple quotes below):\n```{text}```\n\nPlease answer using the entire template below.\n\nTEMPLATE:\nObservation: <The observation on the absence of CONVERSATION WORDS in the TEXT.>\nScore: <The score 0-10 based on the absence of CONVERSATION WORDS.>\nSupporting Evidence: <Provide your reasons for scoring based on the absence of CONVERSATION WORDS. Tie it back to the evaluation being completed.>\n"""\n\nSEARCH_QUERY_ACCURACY_TEMPLATE = """\n\nSystem:\nYou are a SEARCH QUERY ACCURACY classifier providing the accuracy of the SEARCH QUERY formed by combination of USER QUESTION and CHAT HISTORY.\nYou will be provided with a USER QUESTION, CHAT HISTORY and a SEARCH QUERY.\nYour task is to evaluate If the SEARCH QUERY accurately reflects the USER QUESTION and includes all necessary details to be understood on its own. This includes appropriately filling in any gaps left by an incomplete USER QUESTION using the CHAT HISTORY.\nPurpose of the SEARCH QUERY is to retrieve the desired information from the database related to the USER QUESTION.\n\nUSER QUESTION (mentioned within the triple quotes below):\n```{user_question}```\n\nCHAT HISTORY (mentioned within the triple quotes below):\n```{chat_history}```\n\nSEARCH QUERY (mentioned within the triple quotes below):\n```{search_query}```\n\nPlease answer using the entire template below.\n\nTEMPLATE:\nObservation: <The observation on the criteria for the SEARCH QUERY ACCURACY evaluation.>\nScore: <The score 0-10 based on the given criteria>\nSupporting Evidence: <Provide your reasons for scoring based on the ACCURACY of the SEARCH QUERY. Tie it back to the evaluation being completed.>\n"""\n\nSEARCH_QUERY_CONCISENESS_TEMPLATE = """\n\nSystem:\nYou are a SEARCH QUERY CONCISENESS classifier providing the conciseness of the SEARCH QUERY formed by combination of USER QUESTION and CHAT HISTORY.\nYou will be provided with a USER QUESTION, CHAT HISTORY and a SEARCH QUERY.\nYour task is to evaluate If the SEARCH QUERY is free from irrelevant or extraneous information, focusing solely on what is necessary to answer the USER QUESTION effectively.\nPurpose of the SEARCH QUERY is to retrieve the desired information from the database related to the USER QUESTION.\n\nUSER QUESTION (mentioned within the triple quotes below):\n```{user_question}```\n\nCHAT HISTORY (mentioned within the triple quotes below):\n```{chat_history}```\n\nSEARCH QUERY (mentioned within the triple quotes below):\n```{search_query}```\n\nPlease answer using the entire template below.\n\nTEMPLATE:\nObservation: <The observation on the criteria for the SEARCH QUERY CONCISENESS evaluation.>\nScore: <The score 0-10 based on the given criteria>\nSupporting Evidence: <Provide your reasons for scoring based on the CONCISENESS of the SEARCH QUERY. Tie it back to the evaluation being completed.>\n"""\n\n\nclass ExtendedOpenAIProvider(OpenAI):\n\n    def compare_using_conceptual_overlap_with_cot_reasons(\n        self, expected_answer: str, actual_answer: str\n    ) -> float:\n        """Get the comparision feedback using conceptual overlap\n        Uses chat completion Model. Also uses chain of\n        thought methodology and emits the reasons.\n\n        Usage:\n            ``` python\n            feedback = (\n                Feedback(provider.compare_using_conceptual_overlap_with_cot_reasons)\n                .on(expected_answer=Select.RecordCalls.query.args["expected_answer"])\n                .on(actual_answer=Select.RecordCalls.query.rets)\n            )\n\n        Args:\n            expected_answer (str): The expected answer for the current question in test\n            actual_answer (str): The actual answer from the assistant\n\n        Returns:\n            float: A value between 0 and 1. 0 being "not same" and 1 being "mostly same".\n        """\n\n        system_prompt = str.format(\n            COMPARE_ANSWER_TEMPLATE,\n            expected_answer=expected_answer,\n            actual_answer=actual_answer,\n        )\n        system_prompt = system_prompt.replace("Score:", prompts.COT_REASONS_TEMPLATE)\n        return self.generate_score_and_reasons(system_prompt)\n\n    def chat_conversation_presence_with_cot_reasons(self, text: str) -> float:\n        """Get the presence of conversation starters in the text\n\n        Usage:\n            ```python\n            conversation_starters_feedback = (\n                Feedback(provider.chat_conversation_presence_with_cot_reasons, name="Conversation Starters")\n                .on(text=Select.RecordCalls.get_search_query.rets)\n            )\n\n        Args:\n            text (str): The text to be evaluated for conversation starters\n\n        Returns:\n            float: A value between 0 and 1. 0 being "no conversation starters" and 1 being "conversation starters present".\n        """\n\n        system_prompt = str.format(\n            CONVERSATIONAL_WORDS_PRESENCE_TEMPLATE,\n            text=text,\n        )\n        return self.generate_score_and_reasons(system_prompt)\n\n    def search_query_accuracy_with_cot_reasons(\n        self, user_question: str, chat_history: list[str], search_query: str\n    ) -> float:\n        """Get the accuracy score for the search query generated\n\n        Usage:\n            ```python\n            search_query_accuracy_feedback = (\n                Feedback(provider.search_query_accuracy_with_cot_reasons, name="Search Query Accuracy")\n                .on(user_question=Select.RecordCalls.query.args["question"])\n                .on(chat_history=Select.RecordCalls.query.args["chat_history"])\n                .on(search_query=Select.RecordCalls.get_search_query.rets)\n            )\n\n        Args:\n            user_question (str): The user question for which the query is being generated\n            chat_history (list[str]): previous chat history\n            search_query (str): The search query generated by the system\n\n        Returns:\n            float: A value between 0 and 1. 0 being "not accurate" and 1 being "mostly accurate".\n        """\n\n        system_prompt = str.format(\n            SEARCH_QUERY_ACCURACY_TEMPLATE,\n            user_question=user_question,\n            chat_history=chat_history,\n            search_query=search_query,\n        )\n        return self.generate_score_and_reasons(system_prompt)\n\n    def search_query_conciseness_with_cot_reasons(\n        self, user_question: str, chat_history: list[str], search_query: str\n    ) -> float:\n        """Get the conciseness score for the search query generated\n\n        Usage:\n            ```python\n            search_query_conciseness_feedback = (\n                Feedback(provider.search_query_conciseness_with_cot_reasons, name="Search Query Conciseness")\n                .on(user_question=Select.RecordCalls.query.args["question"])\n                .on(chat_history=Select.RecordCalls.query.args["chat_history"])\n                .on(search_query=Select.RecordCalls.get_search_query.rets)\n            )\n\n        Args:\n            user_question (str): The user question for which the query is being generated\n            chat_history (list[str]): previous chat history\n            search_query (str): The search query generated by the system\n\n        Returns:\n            float: A value between 0 and 1. 0 being "not concise" and 1 being "mostly concise".\n        """\n\n        system_prompt = str.format(\n            SEARCH_QUERY_CONCISENESS_TEMPLATE,\n            user_question=user_question,\n            chat_history=chat_history,\n            search_query=search_query,\n        )\n        return self.generate_score_and_reasons(system_prompt)\n'})})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var r=t(96540);const s={},o=r.createContext(s);function a(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);
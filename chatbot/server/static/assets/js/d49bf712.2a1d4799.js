"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[9746],{72041:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var t=i(74848),o=i(28453);const a={},s=void 0,r={id:"AI360/FineTuning/Life cycle of fine tuning/Quantization",title:"Quantization",description:"Quantization vs Parameter Efficient Fine Tuning (peft)",source:"@site/docs/AI360/FineTuning/Life cycle of fine tuning/Quantization.md",sourceDirName:"AI360/FineTuning/Life cycle of fine tuning",slug:"/AI360/FineTuning/Life cycle of fine tuning/Quantization",permalink:"/docs/AI360/FineTuning/Life cycle of fine tuning/Quantization",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"AI360Sidebar",previous:{title:"PEFT",permalink:"/docs/AI360/FineTuning/Life cycle of fine tuning/PEFT"},next:{title:"Libraries and tools",permalink:"/docs/category/libraries-and-tools"}},l={},c=[{value:"<strong>Quantization vs Parameter Efficient Fine Tuning (peft)</strong>",id:"quantization-vs-parameter-efficient-fine-tuning-peft",level:3},{value:"<strong>What is interference?</strong>",id:"what-is-interference",level:3},{value:"<strong>Quantization</strong>",id:"quantization",level:3},{value:"<strong>Methods in the Quantization:</strong>",id:"methods-in-the-quantization",level:4}];function d(n){const e={a:"a",admonition:"admonition",h3:"h3",h4:"h4",hr:"hr",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",strong:"strong",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h3,{id:"quantization-vs-parameter-efficient-fine-tuning-peft",children:(0,t.jsx)(e.strong,{children:"Quantization vs Parameter Efficient Fine Tuning (peft)"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"PEFT is about optimizing the training process, ensuring that the model can be adapted to specific tasks with minimal changes, conserving resources while maintaining performance."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"PEFT is typically used during fine-tuning, where you are adapting a pre-trained model to a specific task."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"PEFT methods update only certain parts of a model while keeping the rest of the model frozen."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Quantization is focused on optimizing the model for inference by making it faster and more memory-efficient, often at the cost of a small drop in model accuracy."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Quantization is generally applied after training, as part of the model optimization for deployment."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Quantization reduces the numerical precision of the entire model, making it lighter and faster during inference."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"They can be combined to get the benefits of both. For example, QLoRA is a technique that applies both quantization (to reduce memory usage) and LoRA (a PEFT method for efficient fine-tuning). This combination allows for efficient fine-tuning on limited hardware while also optimizing the model for efficient deployment."}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"what-is-interference",children:(0,t.jsx)(e.strong,{children:"What is interference?"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Inferencing means load the pretrained model and test your model over the data."}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"quantization",children:(0,t.jsx)(e.strong,{children:"Quantization"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Quantization reduces the model size by changing the memory format of parameters to low format like float to int. Doing quantization will lead to loss of information.Bitsandbytes package is used"}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Symmetric: Scale factor, zero point = 0"}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Asymmetric: Scale factor, zero point != 0 (can be negative)"}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Quantization focuses on optimizing the model for inference by reducing the precision of its weights and activations, thereby enhancing speed and reducing memory usage."}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"By converting high-precision floating-point numbers into lower-precision representations, quantization effectively reduces memory usage and computational requirements while maintaining an acceptable level of accuracy."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.admonition,{type:"tip",children:[(0,t.jsx)(e.mdxAdmonitionTitle,{}),(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"https://medium.com/data-from-the-trenches/quantization-in-llms-why-does-it-matter-7c32d2513c9e",children:"Quantization in LLMs: Why Does It Matter?"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"https://www.maartengrootendorst.com/blog/quantization/",children:"A Visual Guide to Quantization - Maarten Grootendorst"})}),"\n"]}),"\n"]})]}),"\n",(0,t.jsx)(e.h4,{id:"methods-in-the-quantization",children:(0,t.jsx)(e.strong,{children:"Methods in the Quantization:"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Post-Training Quantization (PTQ)","\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"This approach involves quantizing a model after it has been trained.  During this process, weights are typically converted from high-precision formats (like 32-bit floats) to lower precision formats (such as 8-bit integers)."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["Quantization-Aware Training (QAT):","\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"QAT is a method that integrates quantization into the training process itself. By simulating quantization effects during training, the model learns to accommodate the reduced precision of weights and activations."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["QLoRA (Quantized Low-Rank Adaptation):","\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"QLoRA combines quantization with low-rank adaptation techniques. It focuses on fine-tuning large models by quantizing their weights while also introducing low-rank updates for specific tasks. This approach allows for a significant reduction in memory usage and computational costs during fine-tuning, making it highly efficient for adapting large pre-trained models to new tasks."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.li,{children:"Some more new methods of Quantization involves -> GGUF, GGML, GPTQ - Quantization in LLMs: Why Does It Matter? | by Aimee Coelho | data from the trenches | Medium."}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},28453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>r});var t=i(96540);const o={},a=t.createContext(o);function s(n){const e=t.useContext(a);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);
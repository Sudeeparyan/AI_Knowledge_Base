"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[3554],{53637:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var s=t(74848),i=t(28453);const r={},o=void 0,a={id:"RAG360/Generation/Improvements",title:"Improvements",description:"Key Strategies in Generation",source:"@site/docs/RAG360/Generation/Improvements.md",sourceDirName:"RAG360/Generation",slug:"/RAG360/Generation/Improvements",permalink:"/docs/RAG360/Generation/Improvements",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Comparisons",permalink:"/docs/RAG360/Generation/models/Online/Comparisons"},next:{title:"Security",permalink:"/docs/category/security"}},c={},l=[{value:"Key Strategies in Generation",id:"key-strategies-in-generation",level:3},{value:"1. <strong>Context Curation</strong>",id:"1-context-curation",level:4},{value:"2. <strong>Response Synthesis Approaches</strong>",id:"2-response-synthesis-approaches",level:4},{value:"3. <strong>Encoder and LLM Fine-Tuning</strong>",id:"3-encoder-and-llm-fine-tuning",level:4}];function d(e){const n={h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",strong:"strong",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h3,{id:"key-strategies-in-generation",children:"Key Strategies in Generation"}),"\n",(0,s.jsx)(n.p,{children:"To enhance the generation phase in RAG, several strategies can be employed to\nimprove the quality of the output:"}),"\n",(0,s.jsxs)(n.h4,{id:"1-context-curation",children:["1. ",(0,s.jsx)(n.strong,{children:"Context Curation"})]}),"\n",(0,s.jsx)(n.p,{children:"Efficient context curation involves refining the retrieved information to ensure\nthat only the most relevant and concise content is fed to the LLM. This process\nincludes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Reranking"}),": Reordering the document chunks to highlight the most pertinent results first, reducing noise and ensuring the LLM processes the most relevant information."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context Selection/Compression"}),": Using small language models (SLMs) to detect and remove unimportant tokens, transforming the context into a more manageable form for the LLM without sacrificing crucial information."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.h4,{id:"2-response-synthesis-approaches",children:["2. ",(0,s.jsx)(n.strong,{children:"Response Synthesis Approaches"})]}),"\n",(0,s.jsx)(n.p,{children:"This involves generating a response through iterative refinement and\nsummarization:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Iterative Refinement"}),": Sending the retrieved context to the LLM in segments to allow step-by-step refinement of the answer."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Summarization"}),": Summarizing the context to fit within the prompt, ensuring a concise and relevant answer."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multiple Answers and Concatenation"}),": Generating multiple answers from different context chunks and then concatenating or summarizing them to form a cohesive response."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.h4,{id:"3-encoder-and-llm-fine-tuning",children:["3. ",(0,s.jsx)(n.strong,{children:"Encoder and LLM Fine-Tuning"})]}),"\n",(0,s.jsx)(n.p,{children:"Fine-tuning both the Encoder and LLM within the RAG pipeline to improve\nperformance:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Encoder Fine-Tuning"}),": Enhancing the quality of embeddings to improve context retrieval."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ranker Fine-Tuning"}),": Using a cross-encoder to rerank the retrieved results, especially useful when the base Encoder's results lack confidence."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"RA-DIT Technique"}),": Fine-tuning both the LLM and the Retriever using triplets of query, context, and answer, aligning the models more effectively."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(96540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);
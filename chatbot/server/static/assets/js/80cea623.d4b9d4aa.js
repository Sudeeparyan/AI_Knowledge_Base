"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[7631],{20951:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var o=t(74848),s=t(28453);const r={},a=void 0,i={id:"RAG360/Evaluation/Metrics/Answer Relevance",title:"Answer Relevance",description:"Is the answer relevant to the query?",source:"@site/docs/RAG360/Evaluation/Metrics/Answer Relevance.md",sourceDirName:"RAG360/Evaluation/Metrics",slug:"/RAG360/Evaluation/Metrics/Answer Relevance",permalink:"/docs/RAG360/Evaluation/Metrics/Answer Relevance",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Groundedness",permalink:"/docs/RAG360/Evaluation/Metrics/Groundedness"},next:{title:"Tracking",permalink:"/docs/category/tracking"}},l={},c=[{value:"Is the answer relevant to the query?",id:"is-the-answer-relevant-to-the-query",level:3},{value:"<strong>Implementation in Trulens</strong>",id:"implementation-in-trulens",level:3}];function d(e){const n={admonition:"admonition",code:"code",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.admonition,{type:"tip",children:[(0,o.jsx)(n.h3,{id:"is-the-answer-relevant-to-the-query",children:"Is the answer relevant to the query?"}),(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Last, our response still needs to helpfully answer the original question. We\ncan verify this by evaluating the relevance of the final response to the user\ninput."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:'"Usage"'})," part in function documentation tells how to define the\nFeedback Function for Answer Relevance."]}),"\n"]}),"\n"]})]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-in-trulens",children:(0,o.jsx)(n.strong,{children:"Implementation in Trulens"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-js",children:'PR_RELEVANCE = """You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant.\n\nA few additional scoring guidelines:\n\n- Long RESPONSES should score equally well as short RESPONSES.\n\n- Answers that intentionally do not answer the question, such as \'I don\'t know\' and model refusals, should also be counted as the most RELEVANT.\n\n- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\n\n- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n\n- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n\n- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\n\n- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n\n- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\n\n- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\n\n- RESPONSE that confidently FALSE should get a score of 0.\n\n- RESPONSE that is only seemingly RELEVANT should get a score of 0.\n\n- Never elaborate.\n\nPROMPT: {prompt}\n\nRESPONSE: {response}\n\nRELEVANCE:\n"""\n\nCOT_REASONS_TEMPLATE = """\nPlease answer using the entire template below.\n\nTEMPLATE:\nScore: <The score 0-10 based on the given criteria>\nCriteria: <Provide the criteria for this evaluation>\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\n"""\n\ndef relevance_with_cot_reasons(self, prompt: str,\n                                   response: str) -> Tuple[float, Dict]:\n        """\n        Uses chat completion Model. A function that completes a template to\n        check the relevance of the response to a prompt. Also uses chain of\n        thought methodology and emits the reasons.\n\n        **Usage:**\n        ```python\n        feedback = Feedback(provider.relevance_with_cot_reasons).on_input_output()\n\n        The `on_input_output()` selector can be changed. See [Feedback Function\n        Guide](https://www.trulens.org/trulens_eval/feedback_function_guide/)\n\n        Usage on RAG Contexts:\n        ```python\n\n        feedback = Feedback(provider.relevance_with_cot_reasons).on_input().on(\n            TruLlama.select_source_nodes().node.text # See note below\n        ).aggregate(np.mean)\n\n        The `on(...)` selector can be changed. See [Feedback Function Guide :\n        Selectors](https://www.trulens.org/trulens_eval/feedback_function_guide/#selector-details)\n\n        Args:\n            prompt (str): A text prompt to an agent.\n            response (str): The agent\'s response to the prompt.\n\n        Returns:\n            float: A value between 0 and 1. 0 being "not relevant" and 1 being\n            "relevant".\n        """\n        system_prompt = str.format(\n            prompts.PR_RELEVANCE, prompt=prompt, response=response\n        )\n        system_prompt = system_prompt.replace(\n            "RELEVANCE:", prompts.COT_REASONS_TEMPLATE\n        )\n        return self.generate_score_and_reasons(system_prompt)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var o=t(96540);const s={},r=o.createContext(s);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);
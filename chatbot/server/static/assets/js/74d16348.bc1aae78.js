"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[118],{92403:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var t=r(74848),s=r(28453);const o={},i=void 0,a={id:"RAG360/Evaluation/Metrics/Groundedness",title:"Groundedness",description:"Is the response supported by the context?",source:"@site/docs/RAG360/Evaluation/Metrics/Groundedness.md",sourceDirName:"RAG360/Evaluation/Metrics",slug:"/RAG360/Evaluation/Metrics/Groundedness",permalink:"/docs/RAG360/Evaluation/Metrics/Groundedness",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Context Relevance",permalink:"/docs/RAG360/Evaluation/Metrics/Context Relevance"},next:{title:"Answer Relevance",permalink:"/docs/RAG360/Evaluation/Metrics/Answer Relevance"}},d={},c=[{value:"Is the response supported by the context?",id:"is-the-response-supported-by-the-context",level:4}];function u(e){const n={admonition:"admonition",code:"code",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.h4,{id:"is-the-response-supported-by-the-context",children:"Is the response supported by the context?"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"After the context is retrieved, it is then formed into an answer by an LLM.\r\nLLMs are often prone to stray from the facts provided, exaggerating or\r\nexpanding to a correct-sounding answer."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"To verify the groundedness of our application, we can separate the response\r\ninto individual claims and independently search for evidence that supports\r\neach within the retrieved context."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:'"Usage"'})," part in function documentation tells how to define the\r\nFeedback Function for Groundedness."]}),"\n"]}),"\n"]})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-js",children:'LLM_GROUNDEDNESS_FULL_SYSTEM = """You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\r\nFor every sentence in the statement, please answer with this template:\r\n\r\nTEMPLATE:\r\nStatement Sentence: <Sentence>,\r\nSupporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\r\nScore: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\r\n"""\r\n\r\nLLM_GROUNDEDNESS_FULL_PROMPT = """Give me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\r\n\r\nSOURCE: {premise}\r\n\r\nSTATEMENT: {hypothesis}\r\n"""\r\n\r\ndef groundedness_measure_with_cot_reasons(\r\n        self, source: str, statement: str\r\n    ) -> Tuple[float, dict]:\r\n        """\r\n        A measure to track if the source material supports each sentence in the statement using an LLM provider.\r\n\r\n        The LLM will process the entire statement at once, using chain of thought methodology to emit the reasons.\r\n\r\n        Usage on RAG Contexts:\r\n        from trulens_eval import Feedback\r\n        from trulens_eval.feedback import Groundedness\r\n        from trulens_eval.feedback.provider.openai import OpenAI\r\n        grounded = feedback.Groundedness(groundedness_provider=OpenAI())\r\n\r\n\r\n        f_groundedness = feedback.Feedback(grounded.groundedness_measure_with_cot_reasons).on(\r\n            Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content # See note below\r\n        ).on_output().aggregate(grounded.grounded_statements_aggregator)\r\n        The `on(...)` selector can be changed. See [Feedback Function Guide : Selectors](https://www.trulens.org/trulens_eval/feedback_function_guide/#selector-details)\r\n\r\n\r\n        Args:\r\n            source (str): The source that should support the statement\r\n            statement (str): The statement to check groundedness\r\n\r\n        Returns:\r\n            Tuple[float, dict]: A measure between 0 and 1, where 1 means each sentence is grounded in the source.\r\n        """\r\n        groundedness_scores = {}\r\n        if not isinstance(self.groundedness_provider, LLMProvider):\r\n            raise AssertionError(\r\n                "Only LLM providers are supported for groundedness_measure_with_cot_reasons."\r\n            )\r\n        else:\r\n            reason = self.groundedness_provider._groundedness_doc_in_out(\r\n                source, statement\r\n            )\r\n            i = 0\r\n            for line in reason.split(\'\\n\'):\r\n                if "Score" in line:\r\n                    groundedness_scores[f"statement_{i}"\r\n                                       ] = re_0_10_rating(line) / 10\r\n                    i += 1\r\n        return groundedness_scores, {"reasons": reason}\r\n\r\ndef _groundedness_doc_in_out(self, premise: str, hypothesis: str) -> str:\r\n        """\r\n        An LLM prompt using the entire document for premise and entire statement\r\n        document for hypothesis.\r\n\r\n        Args:\r\n            premise (str): A source document\r\n            hypothesis (str): A statement to check\r\n\r\n        Returns:\r\n            str: An LLM response using a scorecard template\r\n        """\r\n        assert self.endpoint is not None, "Endpoint is not set."\r\n\r\n        return self.endpoint.run_in_pace(\r\n            func=self._create_chat_completion,\r\n            prompt=str.format(prompts.LLM_GROUNDEDNESS_FULL_SYSTEM,) +\r\n            str.format(\r\n                prompts.LLM_GROUNDEDNESS_FULL_PROMPT,\r\n                premise=premise,\r\n                hypothesis=hypothesis\r\n            )\r\n        )\r\n        ```\n'})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var t=r(96540);const s={},o=t.createContext(s);function i(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[9013],{51973:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var i=t(74848),r=t(28453);const s={},a=void 0,o={id:"RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant",title:"Qdrant",description:"Hybrid search for Qdrant vector store with v>=10",source:"@site/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant.md",sourceDirName:"RAG360/Retrieval/Unstructured/Searching/Hybrid Search",slug:"/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant",permalink:"/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Overview",permalink:"/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Overview"},next:{title:"Fusion Retriver",permalink:"/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Fusion Retriver"}},d={},c=[{value:"Hybrid search for Qdrant vector store with v&gt;=10",id:"hybrid-search-for-qdrant-vector-store-with-v10",level:4},{value:"Support for multivectors",id:"support-for-multivectors",level:3},{value:"Dense-uint8",id:"dense-uint8",level:3},{value:"Matryoshka vectors",id:"matryoshka-vectors",level:3},{value:"Multi-vector representation",id:"multi-vector-representation",level:3},{value:"Hybrid search",id:"hybrid-search",level:4},{value:"Native support for fusion algorithm",id:"native-support-for-fusion-algorithm",level:4}];function l(e){const n={a:"a",admonition:"admonition",h3:"h3",h4:"h4",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h4,{id:"hybrid-search-for-qdrant-vector-store-with-v10",children:"Hybrid search for Qdrant vector store with v>=10"}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Qdrant Client >=1.10"}),"\n",(0,i.jsx)(n.li,{children:"Fastembed >= 0.3.4"}),"\n",(0,i.jsx)(n.li,{children:"Llama-index-vector-stores-qdrant >=0.2.14"}),"\n"]})}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Hybrid search is Qdrant is revamped and it can be customized as per the\nproject requirement."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Qdrant 1.10 introduces a new Query API that lets you build a search system by\ncombining different search methods to improve retrieval quality."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["This\n",(0,i.jsx)(n.a,{href:"https://qdrant.tech/documentation/concepts/search/#query-api",children:"Query API"}),"\nprovides a single interface for different kinds of searches like search by\nId, similaity saerch, hybrid search, multi stage search etc.\n",(0,i.jsx)(n.img,{alt:"image.png",src:t(74022).A+"",width:"926",height:"348"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"support-for-multivectors",children:"Support for multivectors"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Earlier Qdrant supports only dense and sparse vectors."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["With the latest version 1.10, it supports different vectors like dense-uint8,\ndense, sparse, matryoshka vectors and multi-vector representation.\n",(0,i.jsx)(n.img,{alt:"image.png",src:t(90624).A+"",width:"1090",height:"541"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"dense-uint8",children:"Dense-uint8"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Dense vectors are typically floating point vectors and each element in the\nvector is represented 32 or 64 bits."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"E.g text-embedding-ada-002 is f32 bits."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"But this dense-uint8 are the quantized version of regular dense vectors in\nwhich each element is represented by 8-bit unsigned integer."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"This type of vectors are used to reduce memory usage, but it's precision is\nreduced compared to the floating point vectors."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"matryoshka-vectors",children:"Matryoshka vectors"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The dimension of the embedding vector represents how many numerical values\nare used to represent each text input."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Each dimension in the vector captures certain features or aspects of the text\nlike grammars, sematic features etc.."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Default dimensions of popular text embedding models"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"nomic-embed-text - 768"}),"\n",(0,i.jsx)(n.li,{children:"text-embedding-ada-002 - 1536"}),"\n",(0,i.jsx)(n.li,{children:"text-embedding-3-small - 1536"}),"\n",(0,i.jsx)(n.li,{children:"text-embedding-3-large - 3072"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The high-dimensional vector space allows the embedding to encode rich\ninformation about the text."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The above statement is valid until the introduction of Matryoshka\nRepresentation Learning."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Matryoshka representation learning aims to create hierarchical, nested\nrepresentations that encapsulate multiple levels of information within a\nsingle vector."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"In contrast to common vector embeddings, where all dimensions are equally\nimportant, in Matryoshka embeddings, earlier dimensions store more\ninformation than dimensions later on in the vector, which simply adds more\ndetails. :::note You can think of this by the analogy of trying to classify\nan image at multiple resolutions: The lower resolutions give more high-level\ninformation, while the higher resolutions add more details. :::"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Nomic embed text, text-embedding-3-small and text-embedding-3-large uses\nMatryoshka representation learning and\n",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/embeddings/use-cases",children:"OpenAI"})," also\nreported that text-embedding-3-large embedding can be shortened to a size of\n256 while still outperforming an unshortened text-embedding-ada-002 embedding\nwith a size of 1536 on the MTEB benchmark."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"With this new representation technique we store Matryoshka vectors with\ndifferent dimension sizes and filter that in each stages."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["This Matryoshka vectors is also supported in\n",(0,i.jsx)(n.a,{href:"https://weaviate.io/blog/openais-matryoshka-embeddings-in-weaviate#:~:text=%E2%80%8B,and%20search%20for%20it%20faster",children:"Weaviate"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"multi-vector-representation",children:"Multi-vector representation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A dense vector is typically a single, high-dimensional vector representation\nof a piece of text (word, sentence, paragraph, or document). This vector is\ndense because most of its components are non-zero, capturing rich semantic\ninformation about the text. Each element in a dense vector is a continuous\nvalue, and together, these values form a vector that resides in a\nhigh-dimensional space."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Multi-vector representation involves multiple vectors representing different\nentities (queries, documents, etc..)"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"These multi-vectors are created by specialized models called\nlate-interaction models."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"E.g ColBERT - Contextualized Late Interaction over BERT"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"hybrid-search",children:"Hybrid search"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Using this support for multiple vectors we can customize the hybrid search\naccordingly specific to the use case.\n",(0,i.jsx)(n.img,{alt:"image.png",src:t(2845).A+"",width:"1360",height:"629"})]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"We can use retrieve vectors in of different levels and re-ranking it Late\ninteraction models to get better results."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["To know more about how to utilize the different vector types and re-ranking, check out this ",(0,i.jsx)(n.a,{href:"https://github.com/qdrant/workshop-ultimate-hybrid-search/tree/main/notebooks",children:"github page"})]})]}),"\n",(0,i.jsx)(n.h4,{id:"native-support-for-fusion-algorithm",children:"Native support for fusion algorithm"}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsx)(n.p,{children:"In the latest Qdrant version 1.10, Qdrant has it's built in fusion\nalgorithm - Reciprocal Rank fusion."}),(0,i.jsxs)(n.p,{children:["To learn more about the Hybrid search revamp check out this\n",(0,i.jsx)(n.a,{href:"https://qdrant.tech/articles/hybrid-search/",children:"blog"})]})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},2845:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/hybridsearchQdrant-282e4e4a046f080ef5d44af3388a8ccc.png"},90624:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/multivectoreqdrant-f7c2f589633b3dc812c2a402f46854c0.png"},74022:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/qdrantllamaindex-c84a78c984f0373555a27b6022f10b8a.png"},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(96540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);
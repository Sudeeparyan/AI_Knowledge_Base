"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[9013],{51973:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var t=r(74848),i=r(28453);const s={},a=void 0,o={id:"RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant",title:"Qdrant",description:"Hybrid search for Qdrant vector store with v>=10",source:"@site/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant.md",sourceDirName:"RAG360/Retrieval/Unstructured/Searching/Hybrid Search",slug:"/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant",permalink:"/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Qdrant",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Overview",permalink:"/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Overview"},next:{title:"Fusion Retriver",permalink:"/docs/RAG360/Retrieval/Unstructured/Searching/Hybrid Search/Fusion Retriver"}},d={},c=[{value:"Hybrid search for Qdrant vector store with v&gt;=10",id:"hybrid-search-for-qdrant-vector-store-with-v10",level:4},{value:"Support for multivectors",id:"support-for-multivectors",level:3},{value:"Dense-uint8",id:"dense-uint8",level:3},{value:"Matryoshka vectors",id:"matryoshka-vectors",level:3},{value:"Multi-vector representation",id:"multi-vector-representation",level:3},{value:"Hybrid search",id:"hybrid-search",level:4},{value:"Native support for fusion algorithm",id:"native-support-for-fusion-algorithm",level:4}];function l(e){const n={a:"a",admonition:"admonition",h3:"h3",h4:"h4",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h4,{id:"hybrid-search-for-qdrant-vector-store-with-v10",children:"Hybrid search for Qdrant vector store with v>=10"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Qdrant Client >=1.10"}),"\n",(0,t.jsx)(n.li,{children:"Fastembed >= 0.3.4"}),"\n",(0,t.jsx)(n.li,{children:"Llama-index-vector-stores-qdrant >=0.2.14"}),"\n"]})}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Hybrid search is Qdrant is revamped and it can be customized as per the\r\nproject requirement."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Qdrant 1.10 introduces a new Query API that lets you build a search system by\r\ncombining different search methods to improve retrieval quality."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["This\r\n",(0,t.jsx)(n.a,{href:"https://qdrant.tech/documentation/concepts/search/#query-api",children:"Query API"}),"\r\nprovides a single interface for different kinds of searches like search by\r\nId, similaity saerch, hybrid search, multi stage search etc.\r\n",(0,t.jsx)(n.img,{alt:"image.png",src:r(63367).A+"",width:"926",height:"348"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"support-for-multivectors",children:"Support for multivectors"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Earlier Qdrant supports only dense and sparse vectors."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["With the latest version 1.10, it supports different vectors like dense-uint8,\r\ndense, sparse, matryoshka vectors and multi-vector representation.\r\n",(0,t.jsx)(n.img,{alt:"image.png",src:r(55201).A+"",width:"1090",height:"541"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"dense-uint8",children:"Dense-uint8"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Dense vectors are typically floating point vectors and each element in the\r\nvector is represented 32 or 64 bits."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"E.g text-embedding-ada-002 is f32 bits."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"But this dense-uint8 are the quantized version of regular dense vectors in\r\nwhich each element is represented by 8-bit unsigned integer."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"This type of vectors are used to reduce memory usage, but it's precision is\r\nreduced compared to the floating point vectors."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"matryoshka-vectors",children:"Matryoshka vectors"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The dimension of the embedding vector represents how many numerical values\r\nare used to represent each text input."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Each dimension in the vector captures certain features or aspects of the text\r\nlike grammars, sematic features etc.."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Default dimensions of popular text embedding models"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"nomic-embed-text - 768"}),"\n",(0,t.jsx)(n.li,{children:"text-embedding-ada-002 - 1536"}),"\n",(0,t.jsx)(n.li,{children:"text-embedding-3-small - 1536"}),"\n",(0,t.jsx)(n.li,{children:"text-embedding-3-large - 3072"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The high-dimensional vector space allows the embedding to encode rich\r\ninformation about the text."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The above statement is valid until the introduction of Matryoshka\r\nRepresentation Learning."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Matryoshka representation learning aims to create hierarchical, nested\r\nrepresentations that encapsulate multiple levels of information within a\r\nsingle vector."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"In contrast to common vector embeddings, where all dimensions are equally\r\nimportant, in Matryoshka embeddings, earlier dimensions store more\r\ninformation than dimensions later on in the vector, which simply adds more\r\ndetails. :::note You can think of this by the analogy of trying to classify\r\nan image at multiple resolutions: The lower resolutions give more high-level\r\ninformation, while the higher resolutions add more details. :::"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Nomic embed text, text-embedding-3-small and text-embedding-3-large uses\r\nMatryoshka representation learning and\r\n",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/embeddings/use-cases",children:"OpenAI"})," also\r\nreported that text-embedding-3-large embedding can be shortened to a size of\r\n256 while still outperforming an unshortened text-embedding-ada-002 embedding\r\nwith a size of 1536 on the MTEB benchmark."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"With this new representation technique we store Matryoshka vectors with\r\ndifferent dimension sizes and filter that in each stages."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["This Matryoshka vectors is also supported in\r\n",(0,t.jsx)(n.a,{href:"https://weaviate.io/blog/openais-matryoshka-embeddings-in-weaviate#:~:text=%E2%80%8B,and%20search%20for%20it%20faster",children:"Weaviate"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"multi-vector-representation",children:"Multi-vector representation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"A dense vector is typically a single, high-dimensional vector representation\r\nof a piece of text (word, sentence, paragraph, or document). This vector is\r\ndense because most of its components are non-zero, capturing rich semantic\r\ninformation about the text. Each element in a dense vector is a continuous\r\nvalue, and together, these values form a vector that resides in a\r\nhigh-dimensional space."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Multi-vector representation involves multiple vectors representing different\r\nentities (queries, documents, etc..)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"These multi-vectors are created by specialized models called\r\nlate-interaction models."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"E.g ColBERT - Contextualized Late Interaction over BERT"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"hybrid-search",children:"Hybrid search"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Using this support for multiple vectors we can customize the hybrid search\r\naccordingly specific to the use case.\r\n",(0,t.jsx)(n.img,{alt:"image.png",src:r(41800).A+"",width:"1360",height:"629"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"We can use retrieve vectors in of different levels and re-ranking it Late\r\ninteraction models to get better results."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.mdxAdmonitionTitle,{}),(0,t.jsxs)(n.p,{children:["To know more about how to utilize the different vector types and re-ranking, check out this ",(0,t.jsx)(n.a,{href:"https://github.com/qdrant/workshop-ultimate-hybrid-search/tree/main/notebooks",children:"github page"})]})]}),"\n",(0,t.jsx)(n.h4,{id:"native-support-for-fusion-algorithm",children:"Native support for fusion algorithm"}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.mdxAdmonitionTitle,{}),(0,t.jsx)(n.p,{children:"In the latest Qdrant version 1.10, Qdrant has it's built in fusion\r\nalgorithm - Reciprocal Rank fusion."}),(0,t.jsxs)(n.p,{children:["To learn more about the Hybrid search revamp check out this\r\n",(0,t.jsx)(n.a,{href:"https://qdrant.tech/articles/hybrid-search/",children:"blog"})]})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},41800:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/hybridsearchQdrant-282e4e4a046f080ef5d44af3388a8ccc.png"},55201:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/multivectoreqdrant-f7c2f589633b3dc812c2a402f46854c0.png"},63367:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/qdrantllamaindex-c84a78c984f0373555a27b6022f10b8a.png"},28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(96540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);
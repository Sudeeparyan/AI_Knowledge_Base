"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[4382],{48241:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});var r=s(74848),i=s(28453);const l={},a=void 0,t={id:"RAG360/Generation/models/Offline/Ollama/Overview",title:"Overview",description:"Ollama Overview",source:"@site/docs/RAG360/Generation/models/Offline/Ollama/Overview.md",sourceDirName:"RAG360/Generation/models/Offline/Ollama",slug:"/RAG360/Generation/models/Offline/Ollama/Overview",permalink:"/docs/RAG360/Generation/models/Offline/Ollama/Overview",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Ollama",permalink:"/docs/category/ollama"},next:{title:"Llama 3",permalink:"/docs/RAG360/Generation/models/Offline/Ollama/Llama 3"}},o={},d=[{value:"Ollama Overview",id:"ollama-overview",level:3},{value:"Why Ollama is Important",id:"why-ollama-is-important",level:3},{value:"Problem Statement",id:"problem-statement",level:4},{value:"Use Cases",id:"use-cases",level:4},{value:"Benefits",id:"benefits",level:4},{value:"Advantages and Disadvantages",id:"advantages-and-disadvantages",level:3},{value:"Advantages",id:"advantages",level:4},{value:"Disadvantages",id:"disadvantages",level:4},{value:"Installation and Setup Guide for Ollama",id:"installation-and-setup-guide-for-ollama",level:3},{value:"Installation",id:"installation",level:4},{value:"Running Models",id:"running-models",level:3},{value:"Inferencing",id:"inferencing",level:3},{value:"Inferencing Speed",id:"inferencing-speed",level:3},{value:"GPU Support",id:"gpu-support",level:3},{value:"Memory Requirement",id:"memory-requirement",level:3},{value:"<strong>Memory Requirement for Model Loading (Based on Parameters Count &amp; Bitness)</strong>",id:"memory-requirement-for-model-loading-based-on-parameters-count--bitness",level:4},{value:"<strong>Memory Requirement for Storing Intermediate Results (During Inferencing)</strong>",id:"memory-requirement-for-storing-intermediate-results-during-inferencing",level:4}];function c(e){const n={a:"a",code:"code",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h3,{id:"ollama-overview",children:"Ollama Overview"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Ollama is a component of the Offline model within the broader framework of\r\nGeneration in RAG 360."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"It is designed to manage and execute tasks without the need for real-time\r\ninternet connectivity."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Typically, this system leverages stored or precomputed data to perform its\r\nfunctions, making it ideal for environments with limited or unreliable\r\ninternet access."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-ollama-is-important",children:"Why Ollama is Important"}),"\n",(0,r.jsx)(n.h4,{id:"problem-statement",children:"Problem Statement"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"In many scenarios, constant access to cloud services or real-time data\r\nprocessing is not feasible due to connectivity issues or the high cost of\r\ndata transmission."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"This limitation can severely hinder the performance and scalability of\r\nsystems that rely solely on online resources."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Aspect"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Remote Areas"})}),(0,r.jsx)("td",{children:"In geographic regions lacking stable internet service, Ollama enables continued data handling and processing."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Cost-Sensitive Operations"})}),(0,r.jsx)("td",{children:"Reducing reliance on real-time cloud computing can significantly lower operational costs."})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"benefits",children:"Benefits"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Aspect"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Autonomy"})}),(0,r.jsx)("td",{children:"Systems remain operational regardless of internet availability."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Cost Efficiency"})}),(0,r.jsx)("td",{children:"Decreases the dependency on cloud services, reducing operational costs."})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"advantages-and-disadvantages",children:"Advantages and Disadvantages"}),"\n",(0,r.jsx)(n.h4,{id:"advantages",children:"Advantages"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Factor"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Efficiency"})}),(0,r.jsx)("td",{children:"Performs data processing tasks without the need for constant internet connection, optimizing operational time."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Scalability"})}),(0,r.jsx)("td",{children:"Easily scalable as it does not require additional online resources; it can be expanded with more offline capabilities as needed."})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Factor"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Complexity"})}),(0,r.jsx)("td",{children:"May require comprehensive pre-setup and periodic updates to handle new or evolving data scenarios without online support."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Resource Usage"})}),(0,r.jsx)("td",{children:"Initial set-up and updates might consume more resources upfront."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Limitations"})}),(0,r.jsx)("td",{children:"May not be suitable for tasks requiring real-time data updating or immediate internet-based feedback."})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"installation-and-setup-guide-for-ollama",children:"Installation and Setup Guide for Ollama"}),"\n",(0,r.jsx)(n.h4,{id:"installation",children:"Installation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Download Ollama for Windows"})," -\r\n",(0,r.jsx)(n.a,{href:"https://ollama.com/download/windows",children:"https://ollama.com/download/windows"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Find the model and tag to install."})," E.g. For Llama 3 -\r\n",(0,r.jsx)(n.a,{href:"https://ollama.com/library/llama3/tags",children:"https://ollama.com/library/llama3/tags"})]}),"\n",(0,r.jsxs)(n.li,{children:["Open ",(0,r.jsx)(n.code,{children:"cmd"})," and run:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ollama pull llama3\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"running-models",children:"Running Models"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Open ",(0,r.jsx)(n.code,{children:"cmd"})," and run:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ollama run llama3\n"})}),"\n",(0,r.jsx)(n.p,{children:"If the model is not already available, it will be downloaded."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Inference"})," can be done directly through the CLI. To exit, type ",(0,r.jsx)(n.code,{children:"\\bye"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"To stop Ollama:"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ollama models"})," will be automatically served on port ",(0,r.jsx)(n.code,{children:"11434"})," on localhost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Ollama is Local Offline Model.localhost:11434 is used to check where ollama\r\nis running or not"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"inferencing",children:"Inferencing"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Langchain, Llama-index, and LiteLLM"})," have support to access Ollama."]}),"\n",(0,r.jsx)(n.h3,{id:"inferencing-speed",children:"Inferencing Speed"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"System Setup for Ollama"})," - 32GB RAM (Intel CPU)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Observation:"})," Ollama took 3x to 5x inferencing time compared to\r\nOpenAI/gpt-3.5-turbo."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'# Using GPT-3.5-turbo model\r\nresponse_gpt_35 = completion(\r\n    model="gpt-3.5-turbo",\r\n    messages=messages\r\n)\r\n\r\nprint(response_gpt_35["choices"][0].message.content)\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'# Using LLaMA3 model with a local API\r\nresponse_llama3 = completion(\r\n    model="ollama/llama3",\r\n    messages=messages,\r\n    api_base="http://localhost:11434"\r\n)\r\n\r\nprint(response_llama3["choices"][0].message.content)\r\n\n'})}),"\n",(0,r.jsx)(n.h3,{id:"gpu-support",children:"GPU Support"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Ollama supports NVIDIA GPU."})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ollama supports certain AMD GPU Cards"})," as mentioned here -\r\n",(0,r.jsx)(n.a,{href:"https://ollama.com/blog/amd-preview",children:"https://ollama.com/blog/amd-preview"})]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reference"})," -\r\n",(0,r.jsx)(n.a,{href:"https://github.com/ollama/ollama/blob/main/docs/gpu.md",children:"https://github.com/ollama/ollama/blob/main/docs/gpu.md"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"memory-requirement",children:"Memory Requirement"}),"\n",(0,r.jsx)(n.h4,{id:"memory-requirement-for-model-loading-based-on-parameters-count--bitness",children:(0,r.jsx)(n.strong,{children:"Memory Requirement for Model Loading (Based on Parameters Count & Bitness)"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://medium.com/@baicenxiao/some-basic-knowledge-of-llm-parameters-and-memory-estimation-b25c713c3bd8#:~:text=Memory%20Required%20for%20Training%3A%20A,(28%20GB%20*%204)%20.",children:"Medium Article on Memory Requirement"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Memory requirement depends on both the number of parameters and bitness."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example 1"})," - Llama2-70B with 8-bit quantization version:"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Calculation:"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:"(70 * 1000 * 1000 * 1000 / 1024 / 1024 / 1024) ~ 66GB\r\nif we consider (1000/1024)^3 ~= 1.\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example 2"})," - Llama3-8B with 4-bit quantized version (which is the default\r\nversion in Ollama):"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Calculation:"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"8 * (1/2) GB = 4GB\n"})}),"\n",(0,r.jsx)(n.h4,{id:"memory-requirement-for-storing-intermediate-results-during-inferencing",children:(0,r.jsx)(n.strong,{children:"Memory Requirement for Storing Intermediate Results (During Inferencing)"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Only current layer results are needed during inferencing. There is no need to\r\nstore all the intermediate results."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"7b models"})," generally require at least ",(0,r.jsx)(n.strong,{children:"8GB of RAM"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"13b models"})," generally require at least ",(0,r.jsx)(n.strong,{children:"16GB of RAM"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"70b models"})," generally require at least ",(0,r.jsx)(n.strong,{children:"64GB of RAM"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"If you run into issues with higher quantization levels, try using the q4\r\nmodel or shut down any other programs that are using a lot of memory."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Huggingface Expert Suggestion"})," (in one of the deep learning training\r\ncourses) - 1.2x of memory requirement for model loading for overall\r\ninferencing. (Reference -\r\n",(0,r.jsx)(n.a,{href:"https://learn.deeplearning.ai/courses/open-source-models-hugging-face/lesson/2/selecting-models",children:"Hugging Face Course"}),")"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ollama Recommendation"})," - 2x of memory requirement for model loading."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>t});var r=s(96540);const i={},l=r.createContext(i);function a(e){const n=r.useContext(l);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);
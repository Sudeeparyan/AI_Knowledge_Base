"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[9215],{55943:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var o=i(74848),t=i(28453);const s={},a=void 0,l={id:"RAG360/Tracking/Cost Usage",title:"Cost Usage",description:"Token Usage Calculation",source:"@site/docs/RAG360/Tracking/Cost Usage.md",sourceDirName:"RAG360/Tracking",slug:"/RAG360/Tracking/Cost Usage",permalink:"/docs/RAG360/Tracking/Cost Usage",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Tracking for Debugging",permalink:"/docs/RAG360/Tracking/Tracking for Debugging"},next:{title:"Challenges in RAG",permalink:"/docs/category/challenges-in-rag"}},r={},c=[{value:"Token Usage Calculation",id:"token-usage-calculation",level:3},{value:"Llama-Index token tracking",id:"llama-index-token-tracking",level:4},{value:"Langchain token tracking",id:"langchain-token-tracking",level:4},{value:"Cost Usage Calculation",id:"cost-usage-calculation",level:3},{value:"How Langchain is tracking cost",id:"how-langchain-is-tracking-cost",level:4},{value:"Track usage cost in llama-index",id:"track-usage-cost-in-llama-index",level:4},{value:"Methods to Optimize Usage cost",id:"methods-to-optimize-usage-cost",level:3}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h3:"h3",h4:"h4",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h3,{id:"token-usage-calculation",children:"Token Usage Calculation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["The Context window of a LLM model will be represented in terms of tokens (for\r\nexample ",(0,o.jsx)(e.code,{children:"128K tokens"})," for ",(0,o.jsx)(e.code,{children:"gpt-4"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:"If the token limit for a model is exceeded, the prompt cannot be processed.\r\nSo it is important to make sure you are not exceeding the token limit for\r\neach LLM call you make"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.admonition,{title:"Tools that can track tokens",type:"info",children:(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"llama-index"}),"\n",(0,o.jsx)(e.li,{children:"langchain"}),"\n"]})}),"\n",(0,o.jsxs)(e.admonition,{type:"tip",children:[(0,o.jsx)(e.mdxAdmonitionTitle,{}),(0,o.jsx)(e.p,{children:"Both langchain and llama-index provides token tracking\r\nonly for openAI models for now"})]}),"\n",(0,o.jsx)(e.h4,{id:"llama-index-token-tracking",children:"Llama-Index token tracking"}),"\n",(0,o.jsxs)(e.p,{children:["llama index is having a\r\n",(0,o.jsx)(e.a,{href:"https://docs.llamaindex.ai/en/stable/examples/observability/TokenCountingHandler/",children:"TokenCounting Handler"}),"\r\nthat tells the token count for"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"LLM Prompt Tokens"}),"\n",(0,o.jsx)(e.li,{children:"LLM Completion Tokens"}),"\n",(0,o.jsx)(e.li,{children:"Embedding Tokens (if you are using RAG)"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"langchain-token-tracking",children:"Langchain token tracking"}),"\n",(0,o.jsxs)(e.p,{children:["langchain is having\r\n",(0,o.jsx)(e.a,{href:"https://python.langchain.com/v0.1/docs/modules/model_io/llms/token_usage_tracking/",children:"OpenAI Callback"}),"\r\nwhich provides token count like llama-index, in addition to that it can also\r\nprovide usage cost"]}),"\n",(0,o.jsx)(e.h3,{id:"cost-usage-calculation",children:"Cost Usage Calculation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["Lets say you are having a AI system the uses ",(0,o.jsx)(e.code,{children:"gpt-4-turbo"})," to generate\r\nresponse, you are utilizing maximum context (128K tokens) and the completion\r\ntokens is 1K tokens, if that's the case, a single generation would cost\r\n",(0,o.jsx)(e.code,{children:"$1.3"})," dollars and if 100 people are using your system making 10 generations\r\nin a day, then it would cost ",(0,o.jsx)(e.code,{children:"1.3 * 100 * 10 * 30 = $3900"})," in a month Whereas\r\n",(0,o.jsx)(e.code,{children:"gpt-4o"})," is relatively cheaper and provides response as good as\r\n",(0,o.jsx)(e.code,{children:"gpt-4-turbo"}),", if you have switched to ",(0,o.jsx)(e.code,{children:"gpt-4o"})," the total cost for a month\r\nwould be ",(0,o.jsx)(e.code,{children:"$1950"})]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:"To make these calculation, you need to track the usage cost precisely for\r\neach block of your system where LLM call is made"}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:["To make this process easier, langchain provides\r\n",(0,o.jsx)(e.a,{href:"https://python.langchain.com/v0.1/docs/modules/model_io/llms/token_usage_tracking/",children:"OpenAI Callback"}),"\r\nthat tells the cost for a AI workflow"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"how-langchain-is-tracking-cost",children:"How Langchain is tracking cost"}),"\n",(0,o.jsx)(e.p,{children:"For openai models, langchain will maintain a dictionary that says how much it\r\ncosts for each model to consume 1K tokens, after tracking tokens, it will me\r\nmultiplied with the corresponding model's cost"}),"\n",(0,o.jsx)(e.h4,{id:"track-usage-cost-in-llama-index",children:"Track usage cost in llama-index"}),"\n",(0,o.jsxs)(e.p,{children:["llama-index currently doesnot support cost tracking, but with help of langchain\r\nit is possible to track cost of your llama-index application -\r\n",(0,o.jsx)(e.a,{href:"https://github.com/run-llama/llama_index/issues/13443",children:"refer here"})]}),"\n",(0,o.jsx)(e.h3,{id:"methods-to-optimize-usage-cost",children:"Methods to Optimize Usage cost"}),"\n",(0,o.jsx)(e.p,{children:"If you found your AI system is consuming lot of cost and want to reduce it,\r\nthere are several way to do it"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["Reducing prompt length","\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"prompt compression : summarize the prompt with a lower model before\r\ngetting completion from a more performant model"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["Using lower sized models","\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"compromising performance for cost"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["Using openai batch api","\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"50% less cost with increased latency"}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},28453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var o=i(96540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);
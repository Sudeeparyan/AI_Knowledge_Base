"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[1067],{3102:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>d,toc:()=>c});var a=t(74848),s=t(28453);const i={},r=void 0,d={id:"RAG360/Retrieval/Unstructured/Reranking/Model-Based/Chat LLM",title:"Chat LLM",description:"What is Chat LLM?",source:"@site/docs/RAG360/Retrieval/Unstructured/Reranking/Model-Based/Chat LLM.md",sourceDirName:"RAG360/Retrieval/Unstructured/Reranking/Model-Based",slug:"/RAG360/Retrieval/Unstructured/Reranking/Model-Based/Chat LLM",permalink:"/docs/RAG360/Retrieval/Unstructured/Reranking/Model-Based/Chat LLM",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Cross Embedding Models",permalink:"/docs/RAG360/Retrieval/Unstructured/Reranking/Model-Based/Cross Embedding Models"},next:{title:"LLM Reranker",permalink:"/docs/RAG360/Retrieval/Unstructured/Reranking/Model-Based/LLM Reranker"}},l={},c=[{value:"What is Chat LLM?",id:"what-is-chat-llm",level:3},{value:"Context:",id:"context",level:4},{value:"Why We Need Chat LLM",id:"why-we-need-chat-llm",level:3},{value:"Advantages and Disadvantages of Chat LLM",id:"advantages-and-disadvantages-of-chat-llm",level:3},{value:"Advantages",id:"advantages",level:4},{value:"Disadvantages",id:"disadvantages",level:4}];function o(e){const n={h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h3,{id:"what-is-chat-llm",children:"What is Chat LLM?"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Chat LLM refers to a language model architecture that utilizes both retrieval\ntechniques and large-scale machine learning models to generate responses in\nchat applications."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"It is designed to enhance the conversation experience through accurate and\ncontextually relevant responses, pulling information from an unstructured\ndata set."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"context",children:"Context:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Within the Retrieval Augmented Generation (RAG) framework, the Chat LLM\nfunctions as a component that specifically leverages reranking of retrieved\ninformation to ensure the dialogue generated is coherent and closely aligns\nwith the context of the conversation."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"It fits into the broader process of RAG by using the retrieved data to\naugment its generative capabilities, leading to more informed and accurate\noutputs."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"why-we-need-chat-llm",children:"Why We Need Chat LLM"}),"\n",(0,a.jsx)(n.p,{children:"Managing and utilizing vast amounts of unstructured data effectively in\nreal-time conversation applications is challenging. Without a robust system to\nretrieve and utilize relevant information, responses can be less accurate or out\nof context."}),"\n",(0,a.jsx)(n.h3,{id:"advantages-and-disadvantages-of-chat-llm",children:"Advantages and Disadvantages of Chat LLM"}),"\n",(0,a.jsx)(n.h4,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,a.jsx)("thead",{children:(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Factors"}),(0,a.jsx)("th",{children:"Reason"})]})}),(0,a.jsxs)("tbody",{children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("span",{class:"custom-header",children:"Efficiency"})}),(0,a.jsx)("td",{children:"Streamlines the process of generating context-aware responses by quickly retrieving and reranking pertinent information."})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("span",{class:"custom-header",children:"Understanding"})}),(0,a.jsx)("td",{children:"Enhances the capability of chat systems to understand and process user queries with greater depth."})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("span",{class:"custom-header",children:"Scalability"})}),(0,a.jsx)("td",{children:"Adaptable to various scales of data and different application needs without major changes in architecture."})]})]})]}),"\n",(0,a.jsx)(n.h4,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,a.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,a.jsx)("thead",{children:(0,a.jsxs)("tr",{children:[(0,a.jsx)("th",{children:"Factors"}),(0,a.jsx)("th",{children:"Reason"})]})}),(0,a.jsxs)("tbody",{children:[(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("span",{class:"custom-header",children:"Complexity"})}),(0,a.jsx)("td",{children:"Integrating and maintaining a sophisticated model like Chat LLM could be technically challenging."})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("span",{class:"custom-header",children:"Resource Usage"})}),(0,a.jsx)("td",{children:"Requires significant computational resources for training and operation, potentially increasing the cost."})]}),(0,a.jsxs)("tr",{children:[(0,a.jsx)("td",{children:(0,a.jsx)("span",{class:"custom-header",children:"Limitations"})}),(0,a.jsx)("td",{children:"Might still face challenges handling ambiguous queries or extremely niche topics effectively."})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(o,{...e})}):o(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>d});var a=t(96540);const s={},i=a.createContext(s);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);
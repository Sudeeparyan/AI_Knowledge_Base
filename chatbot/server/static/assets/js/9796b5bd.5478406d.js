"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[1355],{89980:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var s=i(74848),t=i(28453);const r={},a=void 0,o={id:"RAG360/Fundamentals of RAG",title:"Fundamentals of RAG",description:"What is RAG",source:"@site/docs/RAG360/Fundamentals of RAG.md",sourceDirName:"RAG360",slug:"/RAG360/Fundamentals of RAG",permalink:"/docs/RAG360/Fundamentals of RAG",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Introduction",permalink:"/docs/RAG360/Introduction"},next:{title:"Types of RAG",permalink:"/docs/category/types-of-rag"}},l={},d=[{value:"What is RAG",id:"what-is-rag",level:2},{value:"Data Ingestion",id:"data-ingestion",level:3},{value:"Retrieval",id:"retrieval",level:3},{value:"Generation",id:"generation",level:3},{value:"Conclusion",id:"conclusion",level:3},{value:"Reference",id:"reference",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",strong:"strong",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"what-is-rag",children:"What is RAG"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Retrieval Augmented Generation (RAG)"})," is an AI concept designed to enhance\nthe performance of Large Language Models (LLMs) by incorporating real-time,\ncontextually relevant information from external sources during the generation\nof responses."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This addresses key limitations of LLMs, such as,"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Limitation of the training dataset of the foundational model."}),"\n",(0,s.jsx)(n.li,{children:"Inconsistencies and gaps in domain-specific or organization-specific\nknowledge."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This mitigates the risk of producing incorrect or fabricated (hallucinated)\nresponses."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"RAG approach enhances the capabilities of language models like GPT-4 by\nbridging the information gap through the incorporation of external databases."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Typically, when a user asks ChatGPT about recent news, the model\u2019s responses\nare limited to the pretraining data, which may not include the most current\nevents."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"RAG addresses this limitation by sourcing up-to-date information from\nexternal repositories, such as news articles, to provide more relevant and\ntimely answers."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This process involves gathering pertinent articles related to the user\u2019s\nquery and combining them with the initial question to form a comprehensive\nprompt."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This method significantly enhances the depth and accuracy of the responses\ngenerated by the language model, ensuring that it can provide well-informed\nand current answers."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Analogy between pretrained model & Retrieval Augmented Generation: Pretrained\nmodel is like studying beforehand and performing in the exam based on what is\nalready learnt."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"On the other hand, RAG is like an open book exam, where no studying\nbeforehand is required, and the student searches for the answer in the book\nbased on the question and answers it during the exam."}),"\n"]}),"\n"]})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{alt:"RAG.png",src:i(86565).A+"",width:"1133",height:"670"})," A representative instance of the RAG\nprocess applied to question answering."]}),"\n",(0,s.jsx)(n.p,{children:"RAG operates through three critical phases:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Data Ingestion"}),"\n",(0,s.jsx)(n.li,{children:"Retrieval"}),"\n",(0,s.jsx)(n.li,{children:"Generation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"data-ingestion",children:"Data Ingestion"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"DataIngestion.png",src:i(93258).A+"",width:"757",height:"237"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Data ingestion process in Retrieval Augmented Generation (RAG) is key to\npreparing data for the model to generate responses."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["It involves three main steps: ",(0,s.jsx)(n.code,{children:"chunking"}),", ",(0,s.jsx)(n.code,{children:"embedding"}),", and ",(0,s.jsx)(n.code,{children:"indexing"}),"."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Chunking"})," involves breaking down documents into smaller, manageable\nsegments based on criteria like size or natural text divisions. This\nallows the model to focus on specific parts of the text, improving its\nability to retrieve relevant information."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Embedding"})," converts these text chunks into vector representations,\ncapturing the essential qualities of the text in a format that is easy for\nthe model to process."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Indexing"})," organizes these vectors into a structured format optimized\nfor fast retrieval, storing them in a searchable database to help the\nmodel quickly find and use the most relevant information."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In short, chunking divides the text, embedding transforms it into vectors,\nand indexing arranges these vectors for efficient retrieval, enabling the\nmodel to generate accurate and contextually appropriate responses."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.mdxAdmonitionTitle,{}),(0,s.jsx)(n.p,{children:"Please note that there are other steps critical to data ingestion that\nare explained in subsequent sections, as this page covers only key information."})]}),"\n",(0,s.jsx)(n.h3,{id:"retrieval",children:"Retrieval"}),"\n",(0,s.jsx)(n.p,{children:"The retrieval component involves the following steps:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Retrieval.png",src:i(74712).A+"",width:"798",height:"282"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"User Query:"}),' When a user submits a natural language query to the Language\nModel (LLM), such as "What are the key components of cloud computing?", the\nsystem begins a sequence of processes to generate an informed response.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Query Conversion:"})," The natural language query is first processed by an\nembedding model, which converts it into a numerical format called an\nembedding or vector representation. This embedding model is aligned with the\none used during the document ingestion phase, ensuring consistency in data\nprocessing."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Vector Comparison:"})," The system then compares the user's query vector with\nthose stored in a pre-constructed index of a knowledge base. This comparison\nuses similarity metrics like cosine similarity to determine how closely the\nquery vector matches the vectors in the index."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top-K Retrieval:"})," Based on these similarities, the system retrieves the\ntop-K documents or passages from the knowledge base that are most relevant\nto the query."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:[(0,s.jsx)(n.strong,{children:"For example"}),", if K is set to 5, the system would select the five most pertinent documents that relate to the components of cloud computing."]})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Data Retrieval:"})," Finally, the system extracts the content from the\nselected top-K documents and presents it in a readable format, providing\ndetailed and contextually relevant information about the user's query."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Through this retrieval process, the LLM gains context from the most relevant\nsections of the knowledge base, allowing it to deliver a well-informed response,\nsuch as detailing the key components of cloud computing, including computing,\nstorage, and networking services."}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.mdxAdmonitionTitle,{}),(0,s.jsx)(n.p,{children:"Please note that the below sections explain different techniques in\nretrieval that can provide better performance in different scenarios."})]}),"\n",(0,s.jsx)(n.h3,{id:"generation",children:"Generation"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Generation.png",src:i(16760).A+"",width:"982",height:"362"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The synthesis phase in Retrieval Augmented Generation (RAG) is similar to how\nlarge language models (LLMs) typically generate responses."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"However, what sets it apart is the addition of extra context from a detailed\nknowledge base."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In this phase, the LLM creates a final answer by combining its usual language\ngeneration abilities with the information it has retrieved."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This makes the response more complete and accurate, as it includes references\nto specific documents or historical data."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"By using both its generative strengths and precise knowledge inputs, the\nmodel delivers a well-informed and reliable response to the user."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In summary, the Retrieval Augmented Generation (RAG) framework improves\ntraditional language models by adding extra steps to make sure the responses\nare accurate and relevant."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The process starts with the ",(0,s.jsx)(n.strong,{children:"data ingestion phase"}),", where documents are\nbroken down into smaller pieces, turned into embeddings, and stored in an\nindex for easy retrieval."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.strong,{children:"retrieval phase"}),", the system uses this index to find the most\nrelevant documents based on similarity when a user asks a question."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Finally, in the ",(0,s.jsx)(n.strong,{children:"synthesis phase"}),", the model combines this retrieved\ninformation with what it already knows to generate a precise and accurate\nresponse."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This approach not only improves the accuracy of the responses but also allows\nfor verification of the sources of information, reducing the need for\nfrequent retraining."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"RAG effectively solves the limitations of traditional language models by\nensuring the generated content is both relevant and accurate."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"It also tackles challenges like data parsing, efficient embedding, and\ngeneralization across different domains, all without needing to train the\nmodel on domain-specific data."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"This makes RAG easy to apply to new domains without additional training."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This overview covers the key aspects of RAG, with further details provided in\nthe following sections."}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"One of the major advantage of the RAG is instead of just guessing an answer, it looks for relevant documents that we have provided first, then uses that information to generate the output which is more accurate and detailed."})}),"\n",(0,s.jsx)(n.h3,{id:"reference",children:"Reference"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Image Source:\n",(0,s.jsx)(n.a,{href:"https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/",children:"https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/"})]}),"\n"]})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},93258:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/DataIngestion-e5d2c2d99d02ead3c68e722bc5d13103.png"},16760:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/Generation-6737a4c13bea582169d5bd3670c5b83d.png"},74712:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/Retrieval-ee0dfe0931f03214b44b71b5d1d267a7.png"},86565:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/workflow-4d7ac9d39bc4d50186e37b8da486189b.png"},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);
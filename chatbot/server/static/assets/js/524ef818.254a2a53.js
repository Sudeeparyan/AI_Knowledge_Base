"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[3538],{91132:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>o});var i=t(74848),s=t(28453);const a={},d=void 0,r={id:"RAG360/Data Ingestion/Embedding/Model/Online/OpenAI",title:"OpenAI",description:"OpenAI Text Embedding Model",source:"@site/docs/RAG360/Data Ingestion/Embedding/Model/Online/OpenAI.md",sourceDirName:"RAG360/Data Ingestion/Embedding/Model/Online",slug:"/RAG360/Data Ingestion/Embedding/Model/Online/OpenAI",permalink:"/docs/RAG360/Data Ingestion/Embedding/Model/Online/OpenAI",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Online",permalink:"/docs/category/online"},next:{title:"Sparse Vector",permalink:"/docs/category/sparse-vector"}},l={},o=[{value:"OpenAI Text Embedding Model",id:"openai-text-embedding-model",level:3},{value:"Text-embedding-ada-002",id:"text-embedding-ada-002",level:4},{value:"<strong>Explanation</strong>",id:"explanation",level:4},{value:"Example",id:"example",level:4},{value:"Text-embedding-3-small &amp; Text-embedding-3-large",id:"text-embedding-3-small--text-embedding-3-large",level:4},{value:"<strong>Explanation</strong>",id:"explanation-1",level:4},{value:"Example",id:"example-1",level:4},{value:"Output",id:"output",level:4},{value:"Example",id:"example-2",level:4},{value:"Output",id:"output-1",level:4},{value:"Example",id:"example-3",level:4},{value:"Output",id:"output-2",level:4},{value:"Why We Need OpenAI&#39;s Text Embedding Model",id:"why-we-need-openais-text-embedding-model",level:3},{value:"<strong>Problem Statement</strong>",id:"problem-statement",level:4},{value:"Advantages and Disadvantages of Ada-002",id:"advantages-and-disadvantages-of-ada-002",level:3},{value:"<strong>Advantages</strong>",id:"advantages",level:4},{value:"<strong>Disadvantage</strong>",id:"disadvantage",level:4},{value:"Reference",id:"reference",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h3:"h3",h4:"h4",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h3,{id:"openai-text-embedding-model",children:"OpenAI Text Embedding Model"}),"\n",(0,i.jsx)(n.p,{children:"OpenAI supports three major text embedding models:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Embedding Model Name"}),(0,i.jsx)(n.th,{children:"Dimension"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"text-embedding-ada-002"}),(0,i.jsx)(n.td,{children:"1536"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"text-embedding-3-small"}),(0,i.jsx)(n.td,{children:"1536"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"text-embedding-3-large"}),(0,i.jsx)(n.td,{children:"3072"})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"text-embedding-ada-002",children:"Text-embedding-ada-002"}),"\n",(0,i.jsx)(n.h4,{id:"explanation",children:(0,i.jsx)(n.strong,{children:"Explanation"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The OpenAI Text Embedding Model Ada-002 is a machine learning model that\r\nconverts text into numerical embeddings, capturing the semantic meanings of\r\nwords, phrases, and context within the text."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"As part of OpenAI's language model suite, Ada-002 provides high-quality\r\nembeddings that support a range of NLP tasks within Retrieval Augmented\r\nGeneration (RAG) systems."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"In the data ingestion phase of RAG, particularly within Online Models,\r\nAda-002 plays a crucial role by transforming textual input into a\r\nmathematical format that algorithms can easily compare and analyze, enabling\r\nefficient document retrieval, content recommendation, and query answering."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"In this embedding model the dimension size is fixed, we cannot change the\r\ndimension while creating the embeddings."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"example",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:"from openai import OpenAI\r\nclient = OpenAI()\r\n\r\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\r\n   text = text.replace(\"\\n\", \" \")\r\n   return client.embeddings.create(input = [text], model=model).data[0].embedding\r\n\r\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\r\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)\n"})}),"\n",(0,i.jsx)(n.p,{children:"To load the data from a saved file, you can run the following:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:"import pandas as pd\r\n\r\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\r\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"text-embedding-3-small--text-embedding-3-large",children:"Text-embedding-3-small & Text-embedding-3-large"}),"\n",(0,i.jsx)(n.h4,{id:"explanation-1",children:(0,i.jsx)(n.strong,{children:"Explanation"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The OpenAI's text-embedding-3-small and text-embedding-3-large is trained\r\nusing\r\n",(0,i.jsx)(n.a,{href:"https://supabase.com/blog/matryoshka-embeddings",children:"Matryoshka Representation Learning"}),"\r\ntechnique."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The API of these models supports a dimensions parameter with which you can\r\ncontrol the length of the resulting embedding."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"This approach can greatly reduce the time taken to create embeddings and time\r\ntaken to retrieve embeddings."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Matryoshka representation learning aims to create hierarchical, nested\r\nrepresentations that encapsulate multiple levels of information within a\r\nsingle vector."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"In contrast to common vector embeddings, where all dimensions are equally\r\nimportant, in Matryoshka embeddings, earlier dimensions store more\r\ninformation than dimensions later on in the vector, which simply adds more\r\ndetails."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"You can think of this by the analogy of trying to classify an image at\r\nmultiple resolutions: The lower resolutions give more high-level information,\r\nwhile the higher resolutions add more details."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"According to OpenAI's MTEB scores, text-embedding-3-large @ 256 dimensions\r\nstill outperforms text-embedding-ada-002 @ 1536 dimensions with an MTEB score\r\nof 62.0 vs 61.0."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you embed the same text with the same model but with a different\r\ndimensions parameter, you can see that the shorter embedding is not the\r\ntruncated version of the longer one."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"example-1",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:'\r\nfrom openai import OpenAI\r\n\r\nopenai = OpenAI()\r\n\r\ndef vectorize(text,\r\n              dimensions,\r\n              model = "text-embedding-3-small"):\r\n   text = text.lower()\r\n\r\n   return openai.embeddings.create(input = [text],\r\n                                   model = model,\r\n                                   dimensions = dimensions).data[0].embedding\r\n\r\nexample_text = "Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks."\r\n\r\nfull = vectorize(example_text, dimensions = 1536)\r\nshort = vectorize(example_text, dimensions = 8)\r\n\r\nprint(full[:8])\r\nprint(short)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"output",children:"Output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:"[\r\n  -0.001463836757466197, -0.0241670124232769, 0.00683123804628849,\r\n  -0.013936602510511875, 0.0320618636906147, 0.00872271228581667,\r\n  0.031053075566887856, 0.021820487454533577,\r\n][\r\n  (-0.025210261344909668,\r\n  -0.41620534658432007,\r\n  0.11764788627624512,\r\n  -0.24001678824424744,\r\n  0.5521708130836487,\r\n  0.15022294223308563,\r\n  0.5347974300384521,\r\n  0.3757933974266052)\r\n];\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"9",children:["\n",(0,i.jsx)(n.li,{children:"However, if you examine their cosine similarity, you can see that they are\r\nvery similar (or even equal due to rounding).If you take an even closer look,\r\nyou can see that the embeddings actually only differ by a scaling factor (of\r\n0.058 in this case)."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"example-2",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:'from sklearn.metrics.pairwise import cosine_similarity\r\n\r\nprint("Cosine similarity: ",cosine_similarity([full[:8]], [short]))\r\n\r\nscale = full[0]/short[0]\r\nprint("Scale: ",scale)\r\nprint([x * scale for x in short])\r\nprint(full[:8])\r\n\n'})}),"\n",(0,i.jsx)(n.h4,{id:"output-1",children:"Output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:"Cosine similarity: array([[1.]])\r\n\r\nScale: 0.05806511632065122\r\n[-0.001463836757466197, -0.0241670118626955, 0.006831238201508919, -0.01393660272831134, 0.03206186249057062, 0.008722712614794586, 0.031053074983168057, 0.021820487334108546]\r\n[-0.001463836757466197, -0.0241670124232769, 0.00683123804628849, -0.013936602510511875, 0.0320618636906147, 0.00872271228581667, 0.031053075566887856, 0.021820487454533577]\r\n\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"10",children:["\n",(0,i.jsx)(n.li,{children:"Additionally, they do not produce compatible embeddings when sliced to the\r\nsame size. And if we need to change the embedding dimension after generating\r\nit we need to normalize the embedding dimensions."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"example-3",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:'large = vectorize(example_text, dimensions = 3072, model = "text-embedding-3-large")\r\nsmall = vectorize(example_text, dimensions = 1536, model = "text-embedding-3-small")\r\nprint("Large: ",large[:1536])\r\nprint("Small: ",small)\r\n\r\nprint("Cosine similarity: ",cosine_similarity([large[:1536]], [small]))\n'})}),"\n",(0,i.jsx)(n.h4,{id:"output-2",children:"Output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:"Large: [0.011070899665355682,   0.014488349668681622, -0.021118611097335815, -0.011152755469083786, 0.011555208824574947, -0.0007622754783369601, ... ]\r\nSmall: [-0.001463836757466197, -0.0241670124232769,    0.00683123804628849,  -0.013936602510511875, 0.0320618636906147,    0.00872271228581667,   ... ]\r\nCosine similarity: array([[-0.00149749]])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"why-we-need-openais-text-embedding-model",children:"Why We Need OpenAI's Text Embedding Model"}),"\n",(0,i.jsx)(n.h4,{id:"problem-statement",children:(0,i.jsx)(n.strong,{children:"Problem Statement"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Effectively understanding and processing natural language is a significant\r\nchallenge due to the intricacies and nuances of human language."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"These models addresses this by providing deep semantic insights into textual\r\ndata, thus enhancing the retrieval capabilities of RAG systems."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"advantages-and-disadvantages-of-ada-002",children:"Advantages and Disadvantages of Ada-002"}),"\n",(0,i.jsx)(n.h4,{id:"advantages",children:(0,i.jsx)(n.strong,{children:"Advantages"})}),"\n",(0,i.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("th",{children:"Factors"}),(0,i.jsx)("th",{children:"Reason"})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("span",{class:"custom-header",children:"Scalability"})}),(0,i.jsx)("td",{children:"Capable of handling vast amounts of text smoothly because it converts complex texts into simple numeric representations that are easy to manipulate and analyze."})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("span",{class:"custom-header",children:"Versatility"})}),(0,i.jsx)("td",{children:"Supports various languages and domains, making it a valuable tool across different geographic and sectoral applications."})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("span",{class:"custom-header",children:"Integration Ease"})}),(0,i.jsx)("td",{children:"Can be seamlessly integrated into existing RAG frameworks to enhance their performance without substantial modification."})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"disadvantage",children:(0,i.jsx)(n.strong,{children:"Disadvantage"})}),"\n",(0,i.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("th",{children:"Factors"}),(0,i.jsx)("th",{children:"Reason"})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("span",{class:"custom-header",children:"API Usage Costs"})}),(0,i.jsx)("td",{children:"OpenAI's embedding models are accessed via API, which incurs costs based on the number of API calls and the amount of text processed. For high-volume usage, this can become expensive, especially when dealing with large datasets or high-frequency requests."})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("span",{class:"custom-header",children:"Data Security"})}),(0,i.jsx)("td",{children:"Since the data is sent over the internet to an external API, there's always a risk of interception during transmission, despite the use of encryption (e.g., HTTPS). Sensitive data could potentially be exposed if not handled correctly."})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"reference",children:"Reference"}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["Reference Link:- ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/embeddings/embedding-models",children:"OpenAI Text Embeddings"})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>r});var i=t(96540);const s={},a=i.createContext(s);function d(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);
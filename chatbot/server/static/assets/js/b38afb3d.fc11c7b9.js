"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[7818],{24188:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>l});var o=n(74848),s=n(28453);const r={},a=void 0,i={id:"RAG360/Evaluation/Metrics/Context Relevance",title:"Context Relevance",description:"Is the retrieved context relevant to the query?",source:"@site/docs/RAG360/Evaluation/Metrics/Context Relevance.md",sourceDirName:"RAG360/Evaluation/Metrics",slug:"/RAG360/Evaluation/Metrics/Context Relevance",permalink:"/docs/RAG360/Evaluation/Metrics/Context Relevance",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"Good Metrics vs Bad Metrics",permalink:"/docs/RAG360/Evaluation/Metrics/Good Metrics vs Bad Metrics"},next:{title:"Groundedness",permalink:"/docs/RAG360/Evaluation/Metrics/Groundedness"}},c={},l=[{value:"Is the retrieved context relevant to the query?",id:"is-the-retrieved-context-relevant-to-the-query",level:3}];function d(e){const t={admonition:"admonition",code:"code",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(t.admonition,{type:"info",children:[(0,o.jsx)(t.h3,{id:"is-the-retrieved-context-relevant-to-the-query",children:"Is the retrieved context relevant to the query?"}),(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"The first step of any RAG application is retrieval; to verify the quality of\nour retrieval, we want to make sure that each chunk of context is relevant to\nthe input query."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"This is critical because this context will be used by the LLM to form an\nanswer, so any irrelevant information in the context could be weaved into a\nhallucination."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"TruLens enables you to evaluate context relevance by using the structure of\nthe serialized record."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsxs)(t.p,{children:["The ",(0,o.jsx)(t.strong,{children:'"Usage"'})," part in function documentation tells how to define the\nFeedback Function for Context Relevance."]}),"\n"]}),"\n"]})]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-js",children:'COT_REASONS_TEMPLATE = """\nPlease answer using the entire template below.\n\nTEMPLATE:\nScore: <The score 0-10 based on the given criteria>\nCriteria: <Provide the criteria for this evaluation>\nSupporting Evidence: <Provide your reasons for scoring based on the listed criteria step by step. Tie it back to the evaluation being completed.>\n"""\n\nQS_RELEVANCE = """You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\nRespond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant.\n\nA few additional scoring guidelines:\n\n- Long STATEMENTS should score equally well as short STATEMENTS.\n\n- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n\n- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n\n- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n\n- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n\n- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n\n- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n\n- Answers that intentionally do not answer the question, such as \'I don\'t know\', should also be counted as the most relevant.\n\n- Never elaborate.\n\nQUESTION: {question}\n\nSTATEMENT: {statement}\n\nRELEVANCE: """\n\ndef qs_relevance_with_cot_reasons(self, question: str,\n                                      statement: str) -> Tuple[float, Dict]:\n        """\n        Uses chat completion model. A function that completes a\n        template to check the relevance of the statement to the question.\n        Also uses chain of thought methodology and emits the reasons.\n\n        **Usage:**\n        feedback = Feedback(provider.qs_relevance_with_cot_reasons).on_input_output()\n        The `on_input_output()` selector can be changed. See [Feedback Function Guide](https://www.trulens.org/trulens_eval/feedback_function_guide/)\n\n        Usage on RAG Contexts:\n        feedback = Feedback(provider.qs_relevance_with_cot_reasons).on_input().on(\n            TruLlama.select_source_nodes().node.text # See note below\n        ).aggregate(np.mean)\n\n        The `on(...)` selector can be changed. See [Feedback Function Guide : Selectors](https://www.trulens.org/trulens_eval/feedback_function_guide/#selector-details)\n\n\n        Args:\n            question (str): A question being asked.\n            statement (str): A statement to the question.\n\n        Returns:\n            float: A value between 0 and 1. 0 being "not relevant" and 1 being "relevant".\n        """\n        system_prompt = str.format(\n            prompts.QS_RELEVANCE, question=question, statement=statement\n        )\n        system_prompt = system_prompt.replace(\n            "RELEVANCE:", prompts.COT_REASONS_TEMPLATE\n        )\n        return self.generate_score_and_reasons(system_prompt)\n        ```\n'})})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>i});var o=n(96540);const s={},r=o.createContext(s);function a(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(r.Provider,{value:t},e.children)}}}]);
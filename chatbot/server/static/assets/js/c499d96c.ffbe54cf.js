"use strict";(self.webpackChunksudeeparyan_knowledgebase=self.webpackChunksudeeparyan_knowledgebase||[]).push([[5936],{20995:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var r=s(74848),t=s(28453);const i={},a=void 0,o={id:"RAG360/Data Ingestion/Embedding/Sparse Vector/TF-TDF",title:"TF-TDF",description:"Creating Sparse Vectors with TF-IDF",source:"@site/docs/RAG360/Data Ingestion/Embedding/Sparse Vector/TF-TDF.md",sourceDirName:"RAG360/Data Ingestion/Embedding/Sparse Vector",slug:"/RAG360/Data Ingestion/Embedding/Sparse Vector/TF-TDF",permalink:"/docs/RAG360/Data Ingestion/Embedding/Sparse Vector/TF-TDF",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"ragSidebar",previous:{title:"BM25",permalink:"/docs/RAG360/Data Ingestion/Embedding/Sparse Vector/BM25"},next:{title:"Splade",permalink:"/docs/RAG360/Data Ingestion/Embedding/Sparse Vector/Splade"}},c={},l=[{value:"Creating Sparse Vectors with TF-IDF",id:"creating-sparse-vectors-with-tf-idf",level:3},{value:"<strong>Introduction to TF-IDF</strong>",id:"introduction-to-tf-idf",level:4},{value:"Why TF-IDF Works: Term Weighting",id:"why-tf-idf-works-term-weighting",level:3},{value:"Calculating Term Frequency (TF)",id:"calculating-term-frequency-tf",level:3},{value:"<strong>Term Frequency (TF)</strong>",id:"term-frequency-tf",level:4},{value:"<strong>Formula</strong>",id:"formula",level:4},{value:"<strong>Example</strong> For Document 1 (&quot;Data science is awesome&quot;):",id:"example-for-document-1-data-science-is-awesome",level:4},{value:"Calculating Inverse Document Frequency (IDF)",id:"calculating-inverse-document-frequency-idf",level:3},{value:"<strong>Inverse Document Frequency (IDF)</strong>",id:"inverse-document-frequency-idf",level:4},{value:"<strong>Formula</strong>",id:"formula-1",level:4},{value:"<strong>Example</strong>",id:"example",level:4},{value:"Calculating TF-IDF Score",id:"calculating-tf-idf-score",level:3},{value:"<strong>TF-IDF Score</strong>",id:"tf-idf-score",level:4},{value:"<strong>Formula</strong>",id:"formula-2",level:4},{value:"<strong>Example</strong>",id:"example-1",level:4},{value:"Creating the Sparse Vector",id:"creating-the-sparse-vector",level:3},{value:"<strong>Sparse Vector Representation</strong>",id:"sparse-vector-representation",level:4},{value:"<strong>Sparsity</strong>",id:"sparsity",level:4},{value:"Technical Details",id:"technical-details",level:3},{value:"<strong>Sparse Representation</strong>",id:"sparse-representation",level:4},{value:"<strong>Dimensionality</strong>",id:"dimensionality",level:4},{value:"<strong>Memory Usage Example</strong>",id:"memory-usage-example",level:3},{value:"<strong>Memory Efficiency:</strong>",id:"memory-efficiency",level:3},{value:"Advantages and Disadvantages of TF-IDF",id:"advantages-and-disadvantages-of-tf-idf",level:3},{value:"<strong>Advantages</strong>",id:"advantages",level:4},{value:"<strong>Disadvantages</strong>",id:"disadvantages",level:4},{value:"Known Limitations of TF-IDF",id:"known-limitations-of-tf-idf",level:3}];function d(e){const n={admonition:"admonition",code:"code",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h3,{id:"creating-sparse-vectors-with-tf-idf",children:"Creating Sparse Vectors with TF-IDF"}),"\n",(0,r.jsx)(n.h4,{id:"introduction-to-tf-idf",children:(0,r.jsx)(n.strong,{children:"Introduction to TF-IDF"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"TF-IDF (Term Frequency-Inverse Document Frequency)"})," is a statistical\r\nmethod used to evaluate the importance of a word in a document relative to a\r\ncollection of documents (corpus)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"It is commonly used in text mining and information retrieval to transform\r\ntext into a numerical representation that reflects the significance of terms."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-tf-idf-works-term-weighting",children:"Why TF-IDF Works: Term Weighting"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"TF-IDF emphasizes terms that are frequent in a document but rare across the\r\ncorpus."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:['For example, in a document about "',(0,r.jsx)(n.code,{children:"solar energy"}),'," the term "',(0,r.jsx)(n.code,{children:"solar"}),'" might\r\nbe frequent, but if it\u2019s uncommon in the overall corpus, TF-IDF assigns it a\r\nhigher weight.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"This weighting method helps to identify the most relevant terms in a\r\ndocument, making it a powerful tool for text analysis."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Term Frequency (TF):"})," Measures how frequently a term appears in a document. The more a term appears, the higher its TF value."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inverse Document Frequency (IDF):"})," Measures how unique a term is across all documents in the corpus. The more unique a term is, the higher its IDF value."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"calculating-term-frequency-tf",children:"Calculating Term Frequency (TF)"}),"\n",(0,r.jsx)(n.h4,{id:"term-frequency-tf",children:(0,r.jsx)(n.strong,{children:"Term Frequency (TF)"})}),"\n",(0,r.jsx)(n.p,{children:"Calculate the frequency of each term in a specific document. This measures how\r\noften a term appears in a document relative to the total number of words in that\r\ndocument."}),"\n",(0,r.jsx)(n.h4,{id:"formula",children:(0,r.jsx)(n.strong,{children:"Formula"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:"TF(t,d) = Number of times term t appears in document d/ Total number of terms in document d\n"})}),"\n",(0,r.jsxs)(n.h4,{id:"example-for-document-1-data-science-is-awesome",children:[(0,r.jsx)(n.strong,{children:"Example"}),' For Document 1 ("Data science is awesome"):']}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'1. TF("data") = 1/4\r\n2. TF("science") = 1/4\r\n3. TF("is") = 1/4\r\n4. TF("awesome") = 1/4\r\n5. TF(all other items) = 0\n'})}),"\n",(0,r.jsx)(n.h3,{id:"calculating-inverse-document-frequency-idf",children:"Calculating Inverse Document Frequency (IDF)"}),"\n",(0,r.jsx)(n.h4,{id:"inverse-document-frequency-idf",children:(0,r.jsx)(n.strong,{children:"Inverse Document Frequency (IDF)"})}),"\n",(0,r.jsx)(n.p,{children:"Calculate how important a term is across the entire corpus. The idea is to\r\ndown-weight terms that appear frequently across many documents because they are\r\nless informative."}),"\n",(0,r.jsx)(n.h4,{id:"formula-1",children:(0,r.jsx)(n.strong,{children:"Formula"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:"IDF(t) = log(Total of documents/ Number of documents containing term t)\n"})}),"\n",(0,r.jsx)(n.h4,{id:"example",children:(0,r.jsx)(n.strong,{children:"Example"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Assume the corpus has 3 documents."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:'For the term "data" which appears in all three documents'}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'IDF("data") = log(3/3) = log(1) = 0 (This word is not informative since it appears in all documents)\n'})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:'For the term "awesome" which appears in only one document'}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'IDF("awesome") = log(3/1) = log(3) = 1.1\n'})}),"\n",(0,r.jsx)(n.h3,{id:"calculating-tf-idf-score",children:"Calculating TF-IDF Score"}),"\n",(0,r.jsx)(n.h4,{id:"tf-idf-score",children:(0,r.jsx)(n.strong,{children:"TF-IDF Score"})}),"\n",(0,r.jsx)(n.p,{children:"Combine the term frequency and inverse document frequency to compute the TF-IDF\r\nscore for each term in each document."}),"\n",(0,r.jsx)(n.h4,{id:"formula-2",children:(0,r.jsx)(n.strong,{children:"Formula"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:"TF-IDF(t,d) = TF(t,d) * IDF(t)\n"})}),"\n",(0,r.jsx)(n.h4,{id:"example-1",children:(0,r.jsx)(n.strong,{children:"Example"})}),"\n",(0,r.jsx)(n.p,{children:"For Document 1:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-js",children:'1. TF-IDF("data") = (1/4) * 0 = 0\r\n2. TF-IDF("awesome") = (1/4) * 1.1 = 0.275\r\n3. TF-IDF(all other items) = 0\n'})}),"\n",(0,r.jsx)(n.h3,{id:"creating-the-sparse-vector",children:"Creating the Sparse Vector"}),"\n",(0,r.jsx)(n.h4,{id:"sparse-vector-representation",children:(0,r.jsx)(n.strong,{children:"Sparse Vector Representation"})}),"\n",(0,r.jsx)(n.p,{children:"For each document, create a vector where each dimension corresponds to a term\r\nfrom the vocabulary, and the value at each dimension is the TF-IDF score for\r\nthat term in that document."}),"\n",(0,r.jsx)(n.h4,{id:"sparsity",children:(0,r.jsx)(n.strong,{children:"Sparsity"})}),"\n",(0,r.jsx)(n.p,{children:"Since most terms in the vocabulary do not appear in any given document, many\r\nentries in the vector will be zero, leading to a sparse representation."}),"\n",(0,r.jsxs)(n.admonition,{type:"note",children:[(0,r.jsx)(n.p,{children:"[Example] The sparse TF-IDF vector for Document 1 might look like this\r\n(with non-zero elements shown):"}),(0,r.jsx)(n.p,{children:"[0, 0.275, 0.275, 0.275, 0, 0, 0, 0, 0, 0]"}),(0,r.jsx)(n.p,{children:'This vector indicates that only a few terms from the vocabulary ("science",\r\n"is", "awesome") have non-zero TF-IDF scores in Document 1.'})]}),"\n",(0,r.jsx)(n.h3,{id:"technical-details",children:"Technical Details"}),"\n",(0,r.jsx)(n.h4,{id:"sparse-representation",children:(0,r.jsx)(n.strong,{children:"Sparse Representation"})}),"\n",(0,r.jsx)(n.p,{children:"TF-IDF produces sparse vectors, where many elements are zero. This sparsity\r\narises because most terms in a document do not appear in others, leading to zero\r\nentries in the vector."}),"\n",(0,r.jsx)(n.h4,{id:"dimensionality",children:(0,r.jsx)(n.strong,{children:"Dimensionality"})}),"\n",(0,r.jsx)(n.p,{children:"The dimensionality of a TF-IDF vector depends on the number of unique terms in\r\nthe corpus. For example, a corpus with 10,000 unique terms will result in a\r\n10,000-dimensional vector for each document, but most entries will be zero."}),"\n",(0,r.jsx)(n.h3,{id:"memory-usage-example",children:(0,r.jsx)(n.strong,{children:"Memory Usage Example"})}),"\n",(0,r.jsx)(n.p,{children:"For a corpus with 1 million documents and 10,000 unique terms:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dense Representation:"})," Storing full vectors for each document would require 10 billion floats."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sparse Representation:"})," TF-IDF\u2019s sparse vectors require significantly less storage, as only non-zero entries (i.e., terms that actually appear in the document) are stored."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"memory-efficiency",children:(0,r.jsx)(n.strong,{children:"Memory Efficiency:"})}),"\n",(0,r.jsx)(n.p,{children:"TF-IDF is memory efficient due to its sparse nature. For large corpora, this\r\nefficiency is crucial, reducing the computational load and storage requirements\r\ncompared to dense representations like word embeddings."}),"\n",(0,r.jsx)(n.h3,{id:"advantages-and-disadvantages-of-tf-idf",children:"Advantages and Disadvantages of TF-IDF"}),"\n",(0,r.jsx)(n.h4,{id:"advantages",children:(0,r.jsx)(n.strong,{children:"Advantages"})}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Factors"}),(0,r.jsx)("th",{children:"Reason"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Simplicity"})}),(0,r.jsx)("td",{children:"TF-IDF is simple to implement and interpret, making it a popular choice for text representation."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Efficiency"})}),(0,r.jsx)("td",{children:"Sparse vectors reduce memory usage and speed up computations in large-scale text processing tasks."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Relevance"})}),(0,r.jsx)("td",{children:"TF-IDF effectively highlights important terms in documents, improving the accuracy of information retrieval tasks."})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"disadvantages",children:(0,r.jsx)(n.strong,{children:"Disadvantages"})}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Factors"}),(0,r.jsx)("th",{children:"Reason"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Vocabulary Size"})}),(0,r.jsx)("td",{children:"The dimensionality of TF-IDF vectors grows with the size of the vocabulary, which can become unwieldy for very large corpora."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Context Ignorance"})}),(0,r.jsx)("td",{children:"TF-IDF does not capture the semantic meaning of words or their context within a document, potentially leading to less accurate representations compared to methods like word embeddings."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Static Weights"})}),(0,r.jsx)("td",{children:"The weights assigned by TF-IDF are static and do not adapt to different contexts or queries, which can limit its effectiveness in dynamic or complex applications."})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"known-limitations-of-tf-idf",children:"Known Limitations of TF-IDF"}),"\n",(0,r.jsxs)("table",{class:"table-size-for-cloud-services",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Factors"}),(0,r.jsx)("th",{children:"Reason"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Context Sensitivity"})}),(0,r.jsx)("td",{children:"TF-IDF does not consider the context in which a term appears, leading to potential misinterpretations in cases where context is crucial for understanding meaning."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Synonym Handling"})}),(0,r.jsx)("td",{children:"TF-IDF treats synonyms as different terms, which can result in redundant features and dilute the importance of key concepts."})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("span",{class:"custom-header",children:"Scalability Issues"})}),(0,r.jsx)("td",{children:"While sparse vectors are memory-efficient, the increasing dimensionality with larger corpora can still pose scalability challenges, particularly in terms of computational resources."})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var r=s(96540);const t={},i=r.createContext(t);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);